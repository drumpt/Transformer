{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4LUnjN2gquD"
   },
   "source": [
    "## Import libraries and choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3743,
     "status": "ok",
     "timestamp": 1617161510271,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "cbhFmz90dJkm",
    "outputId": "ad59a3c3-826d-4861-d88c-8c1b230f7a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda for torch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import math\n",
    "import easydict\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.dataloader import load_data, get_loader\n",
    "from dataset.field import Vocab\n",
    "from utils import seq2sen\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Use {device} for torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snOczaHygMn1"
   },
   "source": [
    "## Define Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3729,
     "status": "ok",
     "timestamp": 1617161510271,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "DShYHtg1gNEP"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_dim, h, N):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.h = h\n",
    "        self.N = N\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(EncoderIdenticalLayer(self.model_dim, self.h)) for _ in range(self.N)]).to(device)\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "class EncoderIdenticalLayer(nn.Module):\n",
    "    def __init__(self, model_dim, h):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.h = h\n",
    "\n",
    "        self.w_i_Q_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_K_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_V_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_O = nn.Linear(in_features = self.model_dim, out_features = self.model_dim).to(device)\n",
    "        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n",
    "\n",
    "        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048).to(device)\n",
    "        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512).to(device)\n",
    "        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.1).to(device)\n",
    "        self.softmax = nn.Softmax(dim = 2).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Sublayer 1\n",
    "        ## Multi-Head Attention\n",
    "        splitted_x_list = []\n",
    "        for i in range(self.h):\n",
    "            in_softmax = torch.matmul(\n",
    "                self.w_i_Q_list[i](x),\n",
    "                self.w_i_K_list[i](x).transpose(1, 2)\n",
    "            ) / math.sqrt(self.model_dim // self.h)\n",
    "            in_softmax = in_softmax.masked_fill(src_mask == 0, -1e9)\n",
    "            out_softmax = self.w_i_V_list[i](x)\n",
    "            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n",
    "            splitted_x_list.append(splitted_x)\n",
    "        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n",
    "        multi_head_attention_output = self.dropout(self.w_O(concatenated_x))\n",
    "\n",
    "        ## Add & Layer Normalization\n",
    "        multi_head_attention_output += x\n",
    "        multi_head_attention_output = self.layer_normalization1(multi_head_attention_output)\n",
    "\n",
    "        # Sublayer 2\n",
    "        ## Position-wise Feed-Forward Network\n",
    "        ffn_output = self.feed_forward_network1(multi_head_attention_output)\n",
    "        ffn_output = self.relu(ffn_output)\n",
    "        ffn_output = self.feed_forward_network2(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "\n",
    "        ## Add & Layer Normalization\n",
    "        ffn_output += multi_head_attention_output\n",
    "        ffn_output = self.layer_normalization2(ffn_output)\n",
    "        return ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbqLLbJQgXU_"
   },
   "source": [
    "## Define Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4216,
     "status": "ok",
     "timestamp": 1617161510761,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "lPIVhaJ0gX78"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, model_dim, h, N):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.h = h\n",
    "        self.N = N\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(DecoderIdenticalLayer(self.model_dim, self.h)) for _ in range(self.N)]).to(device)\n",
    "\n",
    "    def forward(self, x, y, src_mask, tgt_mask): # x : decoder input, y : encoder output\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y, src_mask, tgt_mask)\n",
    "        return x\n",
    "\n",
    "class DecoderIdenticalLayer(nn.Module):\n",
    "    def __init__(self, model_dim, h):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.h = h\n",
    "\n",
    "        self.w_i_Q_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_K_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_V_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_O_first = nn.Linear(in_features = self.model_dim, out_features = self.model_dim).to(device)\n",
    "        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n",
    "\n",
    "        self.w_i_Q_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_K_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_i_V_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h).to(device) for _ in range(self.h)]\n",
    "        self.w_O_second = nn.Linear(in_features = self.model_dim, out_features = self.model_dim).to(device)\n",
    "        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n",
    "        \n",
    "        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048).to(device)\n",
    "        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512).to(device)\n",
    "        self.layer_normalization3 = nn.LayerNorm(self.model_dim).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 0.1).to(device)\n",
    "        self.softmax = nn.Softmax(dim = 2).to(device)\n",
    "        self.relu = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, x, y, src_mask, tgt_mask):\n",
    "        # Sublayer 1\n",
    "        ## Masked Multi-Head Attention\n",
    "        splitted_x_list = []\n",
    "        for i in range(self.h):\n",
    "            in_softmax = torch.matmul(\n",
    "                self.w_i_Q_list_first[i](x),\n",
    "                self.w_i_K_list_first[i](x).transpose(1, 2)\n",
    "            ) / math.sqrt(self.model_dim // self.h)\n",
    "            in_softmax = in_softmax.masked_fill(tgt_mask == 0, -1e9)\n",
    "            out_softmax = self.w_i_V_list_first[i](x)\n",
    "            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n",
    "            splitted_x_list.append(splitted_x)\n",
    "        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n",
    "        multi_head_attention_output_first = self.dropout(self.w_O_first(concatenated_x))\n",
    "\n",
    "        ## Add & Layer Normalization\n",
    "        multi_head_attention_output_first += x\n",
    "        multi_head_attention_output_first = self.layer_normalization1(multi_head_attention_output_first)\n",
    "\n",
    "        # Sublayer 2\n",
    "        ## Multi-Head Attention\n",
    "        \"\"\"\n",
    "        queries : come from previous decoder layer\n",
    "        keys, values : come from the output of the encoder\n",
    "        \"\"\"\n",
    "        splitted_x_list = []\n",
    "        for i in range(self.h):\n",
    "            in_softmax = torch.matmul(\n",
    "                self.w_i_Q_list_second[i](multi_head_attention_output_first),\n",
    "                self.w_i_K_list_second[i](y).transpose(1, 2)\n",
    "            ) / math.sqrt(self.model_dim // self.h)\n",
    "            in_softmax = in_softmax.masked_fill(src_mask == 0, -1e9)\n",
    "            out_softmax = self.w_i_V_list_second[i](y)\n",
    "            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n",
    "            splitted_x_list.append(splitted_x)\n",
    "        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n",
    "        multi_head_attention_output_second = self.dropout(self.w_O_second(concatenated_x))\n",
    "\n",
    "        ## Masked Multi-Head Attention\n",
    "        multi_head_attention_output_second += multi_head_attention_output_first\n",
    "        multi_head_attention_output_second = self.layer_normalization2(multi_head_attention_output_second)\n",
    "\n",
    "        # Sublayer 3\n",
    "        ## Feed-Forward Network\n",
    "        ffn_output = self.feed_forward_network1(multi_head_attention_output_second)\n",
    "        ffn_output = self.relu(ffn_output)\n",
    "        ffn_output = self.feed_forward_network2(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "\n",
    "        ## Add & Layer Normalization\n",
    "        ffn_output += multi_head_attention_output_second\n",
    "        ffn_output = self.layer_normalization3(ffn_output)\n",
    "        return ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOinDT4ygmn7"
   },
   "source": [
    "## Define Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4215,
     "status": "ok",
     "timestamp": 1617161510762,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "Jaef9wnGf9Co"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, model_dim, src_vocab_size, tgt_vocab_size, max_length):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.h = 8\n",
    "        self.N = 6\n",
    "\n",
    "        self.positional_encoding_constants = self.get_positional_encoding_constants(model_dim = self.model_dim).to(device)\n",
    "        self.input_embedding = nn.Embedding(num_embeddings = self.src_vocab_size, embedding_dim = self.model_dim, padding_idx = 2).to(device)\n",
    "        self.output_embedding = nn.Embedding(num_embeddings = self.tgt_vocab_size, embedding_dim = self.model_dim, padding_idx = 2).to(device)\n",
    "        self.embedding_dropout = nn.Dropout(p = 0.1).to(device)\n",
    "        self.encoder = Encoder(self.model_dim, self.h, self.N).to(device)\n",
    "        self.decoder = Decoder(self.model_dim, self.h, self.N).to(device)\n",
    "        self.final_linear = nn.Linear(self.model_dim, self.tgt_vocab_size).to(device)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, src_mask, tgt_mask):\n",
    "        encoder_input = self.embedding_dropout(self.positional_encoding(self.input_embedding(encoder_input) * math.sqrt(self.model_dim)))\n",
    "        encoder_output = self.encoder(encoder_input, src_mask)\n",
    "        decoder_output = self.decoder(self.embedding_dropout(self.positional_encoding(self.output_embedding(decoder_input) * math.sqrt(self.model_dim))), encoder_output, src_mask, tgt_mask)\n",
    "        final_output = self.final_linear(decoder_output)\n",
    "        return final_output\n",
    "\n",
    "    def positional_encoding(self, embedded_sentence):\n",
    "        encoding_result = embedded_sentence + self.positional_encoding_constants[:, :embedded_sentence.shape[1], :]\n",
    "        return encoding_result\n",
    "        \n",
    "    def get_positional_encoding_constants(self, model_dim):\n",
    "        # calculate positional encoding constants only once to avoid redundant calculations\n",
    "        positional_encoding_constants = []\n",
    "        for pos in range(self.max_length):\n",
    "            pos_constants = []\n",
    "            for idx in range(model_dim):\n",
    "                if idx % 2 == 0: # idx = 2 * i -> 2 * i = idx\n",
    "                    pos_constants.append(math.sin(pos / (10000 ** (idx / model_dim))))\n",
    "                else: # idx = 2 * i + 1 -> 2 * i = idx - 1\n",
    "                    pos_constants.append(math.cos(pos / (10000 ** ((idx  - 1) / model_dim))))\n",
    "            positional_encoding_constants.append(pos_constants)\n",
    "        return torch.tensor(positional_encoding_constants).unsqueeze(0)\n",
    "    \n",
    "    def save_model(self, output_path, epoch, loss, val_loss):\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        output_filename = os.path.join(output_path, f\"weights_{epoch:03d}_{loss:.4f}_{val_loss:.4f}.pt\")\n",
    "        torch.save(self.state_dict(), output_filename)\n",
    "        return output_filename\n",
    "\n",
    "    def plot(self, output_path, history):\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Accuracy versus Epoch')\n",
    "        plt.plot(history['accuracy'])\n",
    "        plt.plot(history['val_accuracy'])\n",
    "        plt.legend(['accuracy', 'val_accuracy'], loc = 'upper right')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Loss versus Epoch')\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.legend(['loss', 'val_loss'], loc = 'upper right')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, \"training_result.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PN9SGQ7g4na"
   },
   "source": [
    "## Define loss function and some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1617162761367,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "oLL55nlkg4VV",
    "outputId": "5c5c56b1-794f-4cf3-cd8a-14612e67a1b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f62f8c44f50>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_mask(batch, pad_idx, is_target = False): # 0 : unavailable\n",
    "    mask = (batch != pad_idx).unsqueeze(-2).to(device)\n",
    "    if is_target:\n",
    "        target_mask = torch.tensor(np.tril(np.ones((1, batch.size(-1), batch.size(-1)))), dtype = torch.bool).to(device)\n",
    "        mask = mask & target_mask\n",
    "    return mask\n",
    "\n",
    "def get_learning_rate(model_dim, step_num, warmup_steps):\n",
    "    return (model_dim ** (-0.5)) * min(step_num ** (-0.5), step_num * (warmup_steps ** (-1.5)))\n",
    "\n",
    "plt.plot(np.arange(1, 20000), [get_learning_rate(512, i, 4000) for i in range(1, 20000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czg4Glm3jfev"
   },
   "source": [
    "## Define main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1617162763446,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "ZZBkM66Lc7FG"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    src, tgt = load_data(args.path)\n",
    "\n",
    "    src_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n",
    "    src_vocab.load(os.path.join(args.path, 'vocab.en'))\n",
    "    tgt_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n",
    "    tgt_vocab.load(os.path.join(args.path, 'vocab.de'))\n",
    "\n",
    "    sos_idx = 0\n",
    "    eos_idx = 1\n",
    "    pad_idx = 2\n",
    "    max_length = 50\n",
    "\n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    print(src_vocab_size)\n",
    "    print(tgt_vocab_size)\n",
    "    \n",
    "    # Define training parameters\n",
    "    model_dim = 512\n",
    "    warmup_steps = 4000\n",
    "\n",
    "    # Define model\n",
    "    transformer = Transformer(model_dim, src_vocab_size, tgt_vocab_size, max_length)\n",
    "    for param in transformer.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "    if args.test and args.model_path: # test\n",
    "        transformer.load_state_dict(torch.load(args.model_path))\n",
    "        transformer.eval()\n",
    "\n",
    "    # Define optimizer\n",
    "    step_num = 1\n",
    "    learning_rate = get_learning_rate(model_dim, step_num, warmup_steps)\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr = learning_rate)\n",
    "\n",
    "    # Define loss function\n",
    "    train_loss_function = nn.CrossEntropyLoss(ignore_index = pad_idx).to(device)\n",
    "    validation_loss_function = nn.CrossEntropyLoss(ignore_index = pad_idx).to(device)\n",
    "\n",
    "    if not args.test:\n",
    "        train_loader = get_loader(src['train'], tgt['train'], src_vocab, tgt_vocab, batch_size=args.batch_size, shuffle=True)\n",
    "        valid_loader = get_loader(src['valid'], tgt['valid'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n",
    "\n",
    "        history = {\n",
    "            \"loss\" : [],\n",
    "            \"val_loss\" : [],\n",
    "            \"accuracy\" : [],\n",
    "            \"val_accuracy\" : []\n",
    "        }\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            print(f\"Epoch {epoch}/{args.epochs}\")\n",
    "            total_train_size, total_validation_size = 0, 0\n",
    "            epoch_train_loss, epoch_validation_loss = 0, 0\n",
    "            epoch_train_correct, epoch_validation_correct = 0, 0\n",
    "\n",
    "            for src_batch, tgt_batch in tqdm(train_loader):\n",
    "            # for src_batch, tgt_batch in train_loader:\n",
    "                # update learning rate first\n",
    "                learning_rate = get_learning_rate(model_dim, step_num, warmup_steps)\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = learning_rate\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n",
    "                tgt_mask = get_mask(torch.tensor(tgt_batch)[:, :-1], pad_idx, is_target = True)\n",
    "\n",
    "                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch)[:, :-1].to(device), src_mask.to(device), tgt_mask.to(device))\n",
    "                loss = train_loss_function( \\\n",
    "                    torch.tensor(prd_batch.reshape(-1, tgt_vocab_size), dtype = torch.float, requires_grad = True).clone().to(device), \\\n",
    "                    torch.tensor(tgt_batch)[:, 1:].reshape(-1).clone().to(device) \\\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step_num += 1\n",
    "\n",
    "                total_train_size += len(src_batch)\n",
    "                epoch_train_loss += loss.data\n",
    "                epoch_train_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).data[:, 1:].to(device)))\n",
    "\n",
    "                gc.collect()\n",
    "                if \"cuda\" in device:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            epoch_train_loss /= total_train_size\n",
    "            epoch_train_accuracy = epoch_train_correct / total_train_size\n",
    "\n",
    "            history[\"loss\"].append(epoch_train_loss)    \n",
    "            history[\"accuracy\"].append(epoch_train_accuracy)\n",
    "\n",
    "            for src_batch, tgt_batch in tqdm(valid_loader):\n",
    "                src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n",
    "                tgt_mask = get_mask(torch.tensor(tgt_batch)[:, :-1], pad_idx, is_target = True)\n",
    "\n",
    "                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch)[:, :-1].to(device), src_mask.to(device), tgt_mask.to(device))\n",
    "                loss = validation_loss_function( \\\n",
    "                    torch.tensor(prd_batch.reshape(-1, tgt_vocab_size), dtype = torch.float, requires_grad = True).clone().to(device), \\\n",
    "                    torch.tensor(tgt_batch)[:, 1:].reshape(-1).clone().to(device) \\\n",
    "                )\n",
    "\n",
    "                total_validation_size += len(src_batch)\n",
    "                epoch_validation_loss += loss.data\n",
    "                epoch_validation_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).data[:, 1:].to(device)))\n",
    "\n",
    "                gc.collect()\n",
    "                if \"cuda\" in device:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            epoch_validation_loss /= total_validation_size\n",
    "            epoch_validation_accuracy = epoch_validation_correct / total_validation_size\n",
    "\n",
    "            history[\"val_loss\"].append(epoch_validation_loss)\n",
    "            history[\"val_accuracy\"].append(epoch_validation_accuracy)\n",
    "\n",
    "            print(f\"loss : {epoch_train_loss:.6f}, val_loss : {epoch_validation_loss:.6f}\") \n",
    "            print(f\"accuracy : {epoch_train_accuracy:.6f}, val_accuracy : {epoch_validation_accuracy:.6f}\")\n",
    "\n",
    "            transformer.save_model(args.output_path, epoch, epoch_train_loss, epoch_validation_loss)\n",
    "            transformer.plot(args.output_path, history)\n",
    "\n",
    "    else: # test\n",
    "        test_loader = get_loader(src['test'], tgt['test'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n",
    "\n",
    "        pred = []\n",
    "        for src_batch, tgt_batch in tqdm(test_loader):\n",
    "            result_batch = torch.tensor([sos_idx for _ in range(len(src_batch))]).unsqueeze(-1)\n",
    "            for idx in range(max_length):\n",
    "                if idx == max_length - 1:\n",
    "                    pred_batch = torch.full((len(src_batch), 1), eos_idx)\n",
    "                else:\n",
    "                    src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n",
    "                    result_mask = get_mask(torch.tensor(result_batch), pad_idx, is_target = True)\n",
    "\n",
    "                    pred_batch = transformer(torch.tensor(src_batch).to(device), result_batch.to(device), src_mask.to(device), result_mask.to(device))[:, -1, :] # consider only last result\n",
    "                    pred_batch[:, sos_idx] += -1e9 # exclude <sos>\n",
    "                    pred_batch[:, pad_idx] += -1e9 # exclude <pad>\n",
    "                    pred_batch = torch.argmax(pred_batch, dim = -1).unsqueeze(-1)\n",
    "                result_batch = torch.cat((result_batch.to(device), pred_batch.to(device)), dim = -1)\n",
    "            result_batch = result_batch[1:, :] # remove <sos> from the result\n",
    "\n",
    "            max_length = 0\n",
    "            for batch_idx in range(result_batch.shape[0]):\n",
    "                max_length = max(max_length, list(result_batch[batch_idx]).index(eos_idx)) # find <eos>\n",
    "            result_batch = result_batch[:max_length, :].tolist()\n",
    "            pred += seq2sen(result_batch, tgt_vocab)\n",
    "\n",
    "            gc.collect()\n",
    "            if \"cuda\" in deivce:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        with open('results/pred.txt', 'w') as f:\n",
    "            for line in pred:\n",
    "                f.write('{}\\n'.format(line))\n",
    "            f.close()\n",
    "\n",
    "        os.system('bash scripts/bleu.sh results/pred.txt multi30k/test.de.atok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cbk70xZjZdQ"
   },
   "source": [
    "## Run main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58730,
     "status": "ok",
     "timestamp": 1617162822541,
     "user": {
      "displayName": "김창훈",
      "photoUrl": "",
      "userId": "04041019206976312811"
     },
     "user_tz": -540
    },
    "id": "DgIAJkvTjhAB",
    "outputId": "10f92ef7-0839-441a-c4d9-a90cb2bd8c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3935\n",
      "4376\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "80it [00:15,  5.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8589/456975736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m })\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8589/2804019064.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mprd_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 loss = train_loss_function( \\\n\u001b[1;32m     72\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8589/3671322784.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input, decoder_input, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8589/2486979552.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# x : decoder input, y : encoder output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8589/2486979552.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m             in_softmax = torch.matmul(\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_i_Q_list_second\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_head_attention_output_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_i_K_list_second\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             ) / math.sqrt(self.model_dim // self.h)\n\u001b[1;32m     72\u001b[0m             \u001b[0min_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"path\": \"multi30k\",\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"test\": False,\n",
    "    \"output_path\": \"resources\",\n",
    "    \"model_path\": \"resources/weights_010_0.0663_0.0668.pt\"\n",
    "})\n",
    "\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOj6gTpW90Nwe+xleMLDfbJ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
