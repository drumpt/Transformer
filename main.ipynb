{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOj6gTpW90Nwe+xleMLDfbJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7jvG1JwnVI4f"},"source":["## Mount google drive and change directory"]},{"cell_type":"code","metadata":{"id":"di0fxpN9uD1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617161510270,"user_tz":-540,"elapsed":3767,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"f2590360-f944-44ce-e7dc-0f0ca52d5374"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = False)\n","\n","!git clone https://github.com/drumpt/NMT_practice.git/ /content/drive/My\\ Drive/Colab\\ Notebooks/CS495\\ Individual\\ Study/1회차\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/CS495\\ Individual\\ Study/1회차/NMT_practice\n","!pip install -r requirements.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","fatal: destination path '/content/drive/My Drive/Colab Notebooks/CS495 Individual Study/1회차' already exists and is not an empty directory.\n","/content/drive/My Drive/Colab Notebooks/CS495 Individual Study/1회차/NMT_practice\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: kora in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.9.19)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: fastcore in /usr/local/lib/python3.7/dist-packages (from kora->-r requirements.txt (line 4)) (1.3.19)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora->-r requirements.txt (line 4)) (5.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora->-r requirements.txt (line 4)) (19.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora->-r requirements.txt (line 4)) (20.9)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (5.0.5)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (0.7.5)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (54.2.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora->-r requirements.txt (line 4)) (2.6.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->kora->-r requirements.txt (line 4)) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kora->-r requirements.txt (line 4)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora->-r requirements.txt (line 4)) (0.2.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o4LUnjN2gquD"},"source":["## Import libraries and choose device"]},{"cell_type":"code","metadata":{"id":"cbhFmz90dJkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617161510271,"user_tz":-540,"elapsed":3743,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"ad59a3c3-826d-4861-d88c-8c1b230f7a69"},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import math\n","import easydict\n","import gc\n","import copy\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","# import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# from torch.autograd import Variable\n","\n","from dataset.dataloader import load_data, get_loader\n","from dataset.field import Vocab\n","from utils import seq2sen\n","\n","if torch.cuda.is_available():\n","    torch.cuda.set_device(0)\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","print(f\"Use {device} for torch\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Use cuda for torch\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"snOczaHygMn1"},"source":["## Define Encoder"]},{"cell_type":"code","metadata":{"id":"DShYHtg1gNEP","executionInfo":{"status":"ok","timestamp":1617161510271,"user_tz":-540,"elapsed":3729,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        # TODO(completed) : num_identical_layers를 이용할 수 있을까? clone을 이용하면 됨.\n","        self.identical_layer1 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer2 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer3 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer4 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer5 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer6 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","\n","    def forward(self, x, src_mask):\n","        x = self.identical_layer1(x, src_mask)\n","        x = self.identical_layer2(x, src_mask)\n","        x = self.identical_layer3(x, src_mask)\n","        x = self.identical_layer4(x, src_mask)\n","        x = self.identical_layer5(x, src_mask)\n","        x = self.identical_layer6(x, src_mask)\n","        return x\n","\n","class EncoderIdenticalLayer(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.w_i_Q_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","\n","        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048, bias = True).to(device)\n","        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512, bias = True).to(device)\n","        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device) # TODO(completed): 맞나?\n","\n","        self.dropout = nn.Dropout(p = 0.1).to(device)\n","        self.softmax = nn.Softmax(dim = 2).to(device) # TODO(completed): 이게 맞나? 2인 것 같다.\n","        self.relu = nn.ReLU().to(device)\n","\n","    def forward(self, x, src_mask):\n","        ### Sublayer 1\n","        # Multi-Head Attention\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list[i](x),\n","                self.w_i_K_list[i](x).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            in_softmax = in_softmax.masked_fill(src_mask == 0, -1e9)\n","            out_softmax = self.w_i_V_list[i](x)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2) # TODO(completed): 맞나?\n","        multi_head_attention_output = self.dropout(self.w_O(concatenated_x))\n","\n","        # Add & Layer Normalization\n","        multi_head_attention_output += x\n","        multi_head_attention_output = self.layer_normalization1(multi_head_attention_output)\n","\n","        ### Sublayer 2\n","        # Feed-Forward Network\n","        ffn_output = self.feed_forward_network1(multi_head_attention_output)\n","        ffn_output = self.relu(ffn_output)\n","        ffn_output = self.feed_forward_network2(ffn_output)\n","        ffn_output = self.dropout(ffn_output)\n","\n","        # positionwise_output_list = [] # (0, 2, 1)\n","        # for position in range(multi_head_attention_output.shape[1]): # token length\n","        #     positionwise_input = multi_head_attention_output[:, position, :]\n","        #     positionwise_output = self.relu(self.feed_forward_network1(positionwise_input))\n","        #     positionwise_output = self.feed_forward_network2(positionwise_output)\n","        #     positionwise_output_list.append(positionwise_output)\n","        # ffn_output = self.dropout(torch.stack(positionwise_output_list, dim = 1))\n","\n","        # Add & Layer Normalization\n","        ffn_output += multi_head_attention_output\n","        ffn_output = self.layer_normalization2(ffn_output)\n","        return ffn_output"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbqLLbJQgXU_"},"source":["## Define Decoder"]},{"cell_type":"code","metadata":{"id":"lPIVhaJ0gX78","executionInfo":{"status":"ok","timestamp":1617161510761,"user_tz":-540,"elapsed":4216,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.identical_layer1 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer2 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer3 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer4 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer5 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer6 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","\n","    def forward(self, x, y, src_mask, tgt_mask):\n","        x = self.identical_layer1(x, y, src_mask, tgt_mask)\n","        x = self.identical_layer2(x, y, src_mask, tgt_mask)\n","        x = self.identical_layer3(x, y, src_mask, tgt_mask)\n","        x = self.identical_layer4(x, y, src_mask, tgt_mask)\n","        x = self.identical_layer5(x, y, src_mask, tgt_mask)\n","        x = self.identical_layer6(x, y, src_mask, tgt_mask)\n","        return x\n","\n","class DecoderIdenticalLayer(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.w_i_Q_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O_first = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","\n","        self.w_i_Q_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O_second = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","        \n","        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048, bias = True).to(device)\n","        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512, bias = True).to(device)\n","        self.layer_normalization3 = nn.LayerNorm(self.model_dim).to(device)\n","\n","        self.dropout = nn.Dropout(p = 0.1).to(device)\n","        self.softmax = nn.Softmax(dim = 2).to(device)\n","        self.relu = nn.ReLU().to(device)\n","\n","    def forward(self, x, y, src_mask, tgt_mask): # TODO(completed): masking 구현.\n","        ### Sublayer 1\n","        # Masked Multi-Head Attention\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list_first[i](x),\n","                self.w_i_K_list_first[i](x).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            in_softmax = in_softmax.masked_fill(tgt_mask == 0, -1e9)\n","            out_softmax = self.w_i_V_list_first[i](x)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n","        multi_head_attention_output_first = self.dropout(self.w_O_first(concatenated_x))\n","\n","        # Add & Layer Normalization\n","        multi_head_attention_output_first += x\n","        multi_head_attention_output_first = self.layer_normalization1(multi_head_attention_output_first)\n","\n","        ## Sublayer 2\n","        # Multi-Head Attention\n","        \"\"\"\n","        queries : come from previous decoder layer\n","        keys, values : come from the output of the encoder\n","        \"\"\"\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list_second[i](multi_head_attention_output_first),\n","                self.w_i_K_list_second[i](y).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            in_softmax = in_softmax.masked_fill(src_mask == 0, -1e9)\n","            out_softmax = self.w_i_V_list_second[i](y)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n","        multi_head_attention_output_second = self.dropout(self.w_O_second(concatenated_x))\n","\n","        # Masked Multi-Head Attention\n","        multi_head_attention_output_second += multi_head_attention_output_first\n","        multi_head_attention_output_second = self.layer_normalization2(multi_head_attention_output_second)\n","\n","        ### Sublayer 3\n","        # Feed-Forward Network\n","        ffn_output = self.feed_forward_network1(multi_head_attention_output_second)\n","        ffn_output = self.relu(ffn_output)\n","        ffn_output = self.feed_forward_network2(ffn_output)\n","        ffn_output = self.dropout(ffn_output)\n","\n","        # positionwise_output_list = [] # (0, 2, 1)\n","        # for position in range(multi_head_attention_output_second.shape[1]): # token length\n","        #     positionwise_input = multi_head_attention_output_second[:, position, :]\n","        #     positionwise_output = self.relu(self.feed_forward_network1(positionwise_input))\n","        #     positionwise_output = self.feed_forward_network2(positionwise_output)\n","        #     positionwise_output_list.append(positionwise_output)\n","        # ffn_output = self.dropout(torch.stack(positionwise_output_list, dim = 1))\n","\n","        # Add & Layer Normalization\n","        ffn_output += multi_head_attention_output_second\n","        ffn_output = self.layer_normalization3(ffn_output)\n","        return ffn_output"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOinDT4ygmn7"},"source":["## Define Transformer"]},{"cell_type":"code","metadata":{"id":"Jaef9wnGf9Co","executionInfo":{"status":"ok","timestamp":1617161510762,"user_tz":-540,"elapsed":4215,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Transformer(nn.Module):\n","    def __init__(self, model_dim, src_vocab_size, tgt_vocab_size, max_length):\n","        super().__init__()\n","        self.src_vocab_size = src_vocab_size\n","        self.tgt_vocab_size = tgt_vocab_size\n","        self.max_length = max_length\n","        self.model_dim = model_dim\n","        self.h = 8\n","        self.positional_encoding_constants = self.get_positional_encoding_constants()\n","\n","        self.input_embedding = nn.Embedding(self.src_vocab_size, self.model_dim).to(device)\n","        self.output_embedding = nn.Embedding(self.tgt_vocab_size, self.model_dim).to(device)\n","        self.embedding_dropout = nn.Dropout(p = 0.1).to(device)\n","        self.encoder = Encoder(self.model_dim, self.h).to(device)\n","        self.decoder = Decoder(self.model_dim, self.h).to(device)\n","        self.final_linear = nn.Linear(self.model_dim, self.tgt_vocab_size).to(device)\n","\n","    def forward(self, encoder_input, decoder_input, src_mask, tgt_mask): # x : encoder_input, y : decoder_input\n","        encoder_input = self.embedding_dropout(self.positional_encoding(self.input_embedding(encoder_input) * math.sqrt(self.model_dim)))\n","        encoder_output = self.encoder(encoder_input, src_mask)\n","        decoder_output = self.decoder(self.embedding_dropout(self.positional_encoding(self.output_embedding(decoder_input) * math.sqrt(self.model_dim))), encoder_output, src_mask, tgt_mask)\n","        final_output = self.final_linear(decoder_output)\n","        return final_output\n","\n","    def positional_encoding(self, embedded_sentence): # TODO(completed): batch 단위가 아니라 문장 단위로 전달되는지 확인. 안 됨.\n","        encoding_result = embedded_sentence.clone().detach()\n","        for batch_idx in range(encoding_result.shape[0]):\n","            encoding_result[batch_idx] += self.positional_encoding_constants[:encoding_result.shape[1], :]\n","        return encoding_result\n","        # for batch_idx in range(embedded_sentence.shape[0]):\n","        #     embedded_sentence[batch_idx] += self.positional_encoding_constants[:embedded_sentence.shape[1], :]\n","        # return embedded_sentence\n","\n","    def get_positional_encoding_constants(self): # avoid redundant calculations\n","        positional_encoding_constants = []\n","        for pos in range(self.max_length):\n","            pos_constants = []\n","            for idx in range(self.model_dim):\n","                if idx % 2 == 0: # idx = 2 * i -> 2 * i = idx\n","                    pos_constants.append(math.sin(pos / (10000 ** (idx / self.model_dim))))\n","                else: # idx = 2 * i + 1 -> 2 * i = idx - 1\n","                    pos_constants.append(math.cos(pos / (10000 ** ((idx  - 1) / self.model_dim))))\n","            positional_encoding_constants.append(pos_constants)\n","        return torch.tensor(positional_encoding_constants).to(device)\n","    \n","    def save_model(self, output_path, epoch, loss, val_loss):\n","        if not os.path.exists(output_path):\n","            os.makedirs(output_path)\n","\n","        output_filename = os.path.join(output_path, f\"weights_{epoch:03d}_{loss:.4f}_{val_loss:.4f}.pt\")\n","        torch.save(self.state_dict(), output_filename)\n","        return output_filename\n","\n","    def plot(self, output_path, history):\n","        plt.subplot(2, 1, 1)\n","        plt.title('Accuracy versus Epoch')\n","        plt.plot(history['accuracy'])\n","        plt.plot(history['val_accuracy'])\n","        plt.legend(['accuracy', 'val_accuracy'], loc = 'upper right')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","\n","        plt.subplot(2, 1, 2)\n","        plt.title('Loss versus Epoch')\n","        plt.plot(history['loss'])\n","        plt.plot(history['val_loss'])\n","        plt.legend(['loss', 'val_loss'], loc = 'upper right')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(output_path, \"training_result.png\"))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PN9SGQ7g4na"},"source":["## Define loss function and some functions"]},{"cell_type":"code","metadata":{"id":"oLL55nlkg4VV","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1617162761367,"user_tz":-540,"elapsed":663,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"5c5c56b1-794f-4cf3-cd8a-14612e67a1b0"},"source":["# class LabelSmoothingsLoss(nn.Module):\n","#     def __init__(self, num_classes, smoothing, dim, is_train):\n","#         super().__init__()\n","#         self.confidence = 1 - smoothing\n","#         self.smoothing = smoothing\n","#         self.num_classes = num_classes\n","#         self.dim = dim\n","#         self.is_train = is_train\n","\n","#     def forward(self, pred, target, pad_start_idx_list = None):\n","#         pred = torch.log(pred.clone().detach().requires_grad_(True))\n","#         true_dist = torch.zeros_like(pred)\n","#         true_dist.fill_(self.smoothing / (self.num_classes - 1))\n","#         true_dist.scatter_(self.dim, target.data.unsqueeze(self.dim), self.confidence)\n","#         return torch.mean(torch.sum(-true_dist * pred, dim = self.dim))\n","\n","# def get_pad_start_idx_list(tgt_batch):\n","#     pad_start_idx_list = []\n","#     for batch_idx in range(tgt_batch.shape[0]):\n","#         try:\n","#             pad_start_idx = list(tgt_batch[batch_idx]).index(2) # <pad>\n","#         except ValueError:\n","#             pad_start_idx = tgt_batch[batch_idx].shape[0]\n","#         pad_start_idx_list.append(pad_start_idx)\n","#     return pad_start_idx_list\n","\n","def get_mask(batch, pad_idx, is_target = False):\n","    mask = (batch != pad_idx).unsqueeze(-2).to(device)\n","    if is_target:\n","        target_mask = np.triu( \\\n","            np.ones((1, batch.size(-1), batch.size(-1))), \\\n","            k = 1 \\\n","        )\n","        target_mask = torch.from_numpy(target_mask) == 0\n","        mask = mask.to(device) & target_mask.to(device)\n","    return mask\n","\n","def get_learning_rate(model_dim, step_num, warmup_steps):\n","    return (model_dim ** (-0.5)) * min(step_num ** (-0.5), step_num * (warmup_steps ** (-1.5)))\n","\n","plt.plot(np.arange(1, 20000), [get_learning_rate(512, i, 4000) for i in range(1, 20000)])"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7ffac0b80590>]"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Zn48c9DdsgC2SCEQAKEJaiAREBUpIKK2oq2WrG2o3Wh7eh0WmeptJ2ZjlN/Ld2sbbHWKq1aFXFpJ52qkIALWragqHBDIAQkbDcBwg4JSZ7fH/cEr1lvkpvc7Xm/Xnlx7jnf8z3POQn3ued8z32OqCrGGGOMt36BDsAYY0zwseRgjDGmFUsOxhhjWrHkYIwxphVLDsYYY1qJDnQA/pCenq65ubmBDsMYY0LKxo0bD6pqRlvLwiI55ObmUlpaGugwjDEmpIjIx+0ts8tKxhhjWrHkYIwxphVLDsYYY1qx5GCMMaYVSw7GGGNa8Sk5iMhcESkXkQoReaCN5XEi8oKzfJ2I5HotW+jMLxeRqzvrU0RWi8gm52efiPylZ7tojDGmqzq9lVVEooDFwJXAHmCDiBSpqsur2V1AraqOFpH5wCLgFhEpAOYDE4ChQImIjHHWabNPVb3Ma9svA//b4700xhjTJb6cOUwFKlS1UlXrgaXAvBZt5gFPOdMvAbNFRJz5S1W1TlV3AhVOf532KSLJwBWAnTn40dvbati892igwzDGBDlfkkM2UOX1eo8zr802qtoAHAXSOljXlz5vAFaq6rG2ghKRBSJSKiKlNTU1PuyGOdvYxD8sWc9nf/0OR0+fDXQ4xpggFswD0rcCz7e3UFUfV9VCVS3MyGjz29+mhXWVh89N//i1sgBGYowJdr4kh71AjtfrYc68NtuISDSQAhzqYN0O+xSRdDyXnv7my04Y3xS7DhAf049/uHgEz6+vYs2OQ4EOyRgTpHxJDhuAfBHJE5FYPAPMRS3aFAG3O9M3AavU8/zRImC+czdTHpAPrPehz5uA/1PVM93dMfNpqkqxy82lozNYeM14hqf2Z+ErH3LmbGOgQzPGBKFOk4MzhnAfsBwoA5ap6hYReVBErneaPQmkiUgFcD/wgLPuFmAZ4AJeB+5V1cb2+vTa7Hw6uKRkum7LvmPsO3qGqwoGkxAbxY8+fz67Dp3ikZXbAx2aMSYI+VSVVVVfBV5tMe8/vabPADe3s+5DwEO+9Om1bJYvcRnfFbvciMAV4zMBuGR0Ol8sHMbjb1dy9YQhTMoZGOAIjTHBJJgHpI0flZS5mTJ8EOmJcefmfe+6AjKT4rj/hU2crrfLS8aYT1hyiAB7j5xmy75jXFkw+FPzUxJi+PnNE6k8eJIf2d1LxhgvlhwiQInLDcCcFskBYMbodO66NI+n13zMm+XVfR2aMSZIWXKIAMUuNyMzBjAqI7HN5f929VjyMxP595c+pPZkfR9HZ4wJRpYcwtzR02dZW3mo1SUlb/ExUfxy/iRqT9Xz7y9/iOcuZGNMJLPkEObeLK+moUm5qoPkADBhaAoPXDOeYpebJe/u6pvgjDFBy5JDmCspqyY9MZZJOYM6bXvnJblcWTCYH79WxqaqI30QnTEmWFlyCGP1DU28ubWa2eMGE9VPOm0vIvzspokMTo7n3mff4+gpK85nTKSy5BDG1u08xPG6hjbvUmpPSv8YfvOlC6k+foZ/fekDG38wJkJZcghjxS438TH9uHR0epfWm5QzkO9e6xl/WPxGRS9FZ4wJZpYcwpSqUuJyc1l+BgmxUV1e/44Zudw4OZufrdhGsfM9CWNM5LDkEKaaC+11dAtrR0SEH33+fC4YlsK3X9jEdvdxP0dojAlmlhzCVLHLTT+B2eMyu91HfEwUv/vKFOJjorjn6VIboDYmglhyCFPFLjdTRgwizavQXndkpSTw2JcvZO+R09z3/HucbWzyU4TGmGBmySEM7ak9hWv/MeaM794lpZYKc1N56IbzWb39IN//82a7g8mYCODT8xxMaGkutNfd8Ya2fPGiHPbUnuJXqyoYNiiBf5qd77e+jTHBx5JDGCouczMqYwAj2ym0113fvnIMe2pP8/PibQwdmMAXpgzza//GmOBhl5XCzNHTZ1lXeZgrC4b4vW8R4cdfuIAZo9L4zssf8s72g37fhjEmOFhyCDPNhfb8eUnJW2x0Px77yhRGZSTytWdKeX93ba9sxxgTWD4lBxGZKyLlIlIhIg+0sTxORF5wlq8TkVyvZQud+eUicnVnfYrHQyKyTUTKROSbPdvFyFLscjuF9nrvmdDJ8TE8fddU0hLjuOMPG9h64FivbcsYExidJgcRiQIWA9cABcCtIlLQotldQK2qjgYeBhY56xYA84EJwFzgURGJ6qTPO4AcYJyqjgeW9mgPI0h9QxNvldf4XGivJwYnx/Ps3dNIiIniy0+sZ+fBk726PWNM3/LlzGEqUKGqlapaj+fNel6LNvOAp5zpl4DZIiLO/KWqWqeqO4EKp7+O+vwG8KCqNgGoqj270kdrKz2F9nrrklJLOan9+dPdU2lS5ctPrGPvkdN9sl1jTO/zJTlkA1Ver/c489pso6oNwFEgrYN1O+pzFHCLiJSKyGsi0uY9kyKywGlTWlNT48NuhL9il5uEmCguze9aob2eGJ2ZxNN3TuXYmbPc9vu17LMEYUxYCMYB6TjgjKoWAr8HlrTVSFUfV9VCVS3MyMjo0wCDkapSUubmsvx04mO6XmivJ87LTuGpO6dy6EQ9tzy+hj21p/p0+8YY//MlOezFMwbQbJgzr802IhINpACHOli3oz73AK84038GLvAhxoi3Zd8x9veg0F5PXTh8EM/cPY2jp85yy+/WUnXYEoQxocyX5LAByBeRPBGJxTPAXNSiTRFwuzN9E7BKPTUWioD5zt1MeUA+sL6TPv8CfMaZvhzY1r1diywrnEJ7V/Sg0F5PTcoZyHP3TOdEXQO3/G4Nu2yQ2piQ1WlycMYQ7gOWA2XAMlXdIiIPisj1TrMngTQRqQDuBx5w1t0CLANcwOvAvara2F6fTl8/Br4gIh8BPwLu9s+uhjd/FdrrqfOyU3j+numcPtvILY+vYZuV+jYmJEk4FFErLCzU0tLSQIcRMFWHT3HZT97gu9eOY8HMUYEOB4DyA8f5ypPrqGtoYskdhUwZkRrokIwxLYjIRmd8t5VgHJA2XVRS1lxoz/8lM7pr7JAkXv7GDFIHxHLbE+tYtdWeJmdMKLHkEAZKytyMzkwkL31AoEP5lJzU/rz49YvJz0zinqc38vLGPYEOyRjjI0sOIe6TQnuBuUupM+mJcTy/YDrTR6byLy9+wKNvVtjzIIwJAZYcQlxzoT1/PdinNyTGRbPkjou4fuJQfvJ6Of/+0ofUN9gT5YwJZvY8hxC3wuUmPTGOyb1YaM8f4qKjeGT+JPLSB/DIyu18fPgUv/vyFAYNiA10aMaYNtiZQwira2jkrfIa5ozPpF8vF9rzBxHh21eO4ZH5k9hUdYQbH32XHTUnAh2WMaYNlhxC2LrKw5zow0J7/jJvUjbP3zON42cauHHxu7xRbrUVjQk2lhxCWHOhvUtG912hPX+ZMiKVv9x7CdmD+nPnHzfwq5XbaWqygWpjgoUlhxAVyEJ7/pKT2p9XvjGDGyZl84vibSx4ppSjp88GOixjDJYcQtbmvYEttOcvCbFR/OKLE/nv6yfwZnkN837zjj1ZzpggYMkhRBW7DtBPYHYQ38LqKxHh9hm5LF0wnZP1jdyw+F2Wrt9t34cwJoAsOYSoFS43hSNSSQ2jW0ELc1P52z9dypQRg3jglY+47/n3OXbGLjMZEwiWHEJQ1eFTbD1wPOQvKbUlMzmeZ+6cxr9dPZbXNx/g2kdW897u2kCHZUzEseQQgpoL7c0Jw+QA0K+fcO9nRrPsaxejCjc/tobFb1TQaHczGdNnLDmEoGJXcBba87cpIwbx6j9fxtzzhvDT5eV80R4gZEyfseQQYo6eOsu6ncFbaM/fUhJi+M2tk/nlLZPY7j7ONY+s5qm/77LvRBjTyyw5hJg3yqtpbNKISQ7guZvphsnZrPj25UzNS+W/irbw5SfXsafWnlNtTG+x5BBiil1uMpLimDQsuAvt9YYhKfH88asX8aPPn88HVUeY+8vV/Gntx3YWYUwvsOQQQuoaGnlrW+gU2usNIsKtU4fz+rdmcsGwFL7/l83c/Dt7VrUx/uZTchCRuSJSLiIVIvJAG8vjROQFZ/k6Ecn1WrbQmV8uIld31qeI/FFEdorIJudnUs92MXysDdFCe70hJ7U/z949jZ/fPJHKmhNc96vV/Gx5OWfONgY6NGPCQqfJQUSigMXANUABcKuIFLRodhdQq6qjgYeBRc66BcB8YAIwF3hURKJ86PPfVHWS87OpR3sYRopdB0iIiWLGqNArtNcbRIQvTBnGyn+ZxecmDuU3b1Qw95dv827FwUCHZkzI8+XMYSpQoaqVqloPLAXmtWgzD3jKmX4JmC0i4sxfqqp1qroTqHD686VP40VVKXFVM3NM6Bba6y2pA2L5xRcn8ae7pqHAbU+s495n32PvkdOBDs2YkOVLcsgGqrxe73HmtdlGVRuAo0BaB+t21udDIvKhiDwsInFtBSUiC0SkVERKa2pqfNiN0PbR3qMcOHaGKwuGBDqUoHVpfjrLvzWT+68cw8qtbmb//E0eKdlul5qM6YZgHJBeCIwDLgJSge+01UhVH1fVQlUtzMjI6Mv4AqLE5aafwBXjMgMdSlCLj4nim7PzWfkvs5g9fjAPl2xj9s/f4vXN+62QnzFd4Ety2AvkeL0e5sxrs42IRAMpwKEO1m23T1Xdrx51wB/wXIKKeCtcbgpzw6vQXm/KHpjA4i9dyHP3TCMxLpqv/+k9bntiHZv3Hg10aMaEBF+SwwYgX0TyRCQWzwBzUYs2RcDtzvRNwCr1fEwrAuY7dzPlAfnA+o76FJEs518BbgA292QHw8G5QnthUJ67r80Ylc7fvnkp/339BMr2H+Ozv36Hbz7/PlWH7Qt0xnQkurMGqtogIvcBy4EoYImqbhGRB4FSVS0CngSeEZEK4DCeN3ucdssAF9AA3KuqjQBt9els8lkRyQAE2AR83X+7G5qKXZ5Ce3YLa/dER/Xj9hm53HhhNo+9uYMl7+7ktc37+cr0XP7pitEMsrMxY1qRcLgOW1hYqKWlpYEOo9fc+vhaDp6oo/j+ywMdSlg4cPQMDxdv48WNVQyIjebrs0bx1Uty6R/b6WclY8KKiGxU1cK2lgXjgLTxcuRUPet3RU6hvb4wJCWeRTddwOvfmsm0kan8dHk5ly16g9+/XcnperuzyRiw5BD03iyvibhCe31lzOAknrj9Il7+xgwKhibz0KtlXPaTN3hidaXd/moiniWHINdcaG9iBBba6ytTRgzimbum8eLXL2bM4ER++LcyZv7kDf7w7k5LEiZiWXIIYnUNjbxZXh3Rhfb60kW5qTx3z3ReWDCdvPQB/PdfXVy66A1+++YOjtuzrE2EseQQxNbsOMTJ+ka7pNTHpo1M44WvXcxz90xjfFYSi17fyowfr+Inr2+l5nhdoMMzpk/Y7RlBrNjlpn+sFdoLlBmj0pkxKp2P9hzlsbd28Nu3dvDEOzv5YuEwFlw2iuFp/QMdojG9xpJDkGpqUkrK3MzMz7BCewF2/rAUFt92IZU1J/j96kqWbdjDc+t2c835Wdx5SR4XDh+I5zubxoQPSw5BavO+o7iP1dklpSAyMiORH33+Ar41ZwxL3tnJc+t387cP9zNxWApfvSSPa8/PIjbartSa8GB/yUGq2ArtBa3ByfEsvHY8axfO5n/mTeB4XQPfemETly5axa9XbufQCRuXMKHPviEdpOb+8m2SE2JY9rWLAx2K6URTk/L29hqWvLuLt7fVEBvdj+snDuW2acOZlGOXnEzw6ugb0nZZKQg1F9r7/nXjAx2K8UG/fsKssZnMGptJRfVx/vj3Xbzy3l5e2riHgqxkbps+nHmTskmMs/9uJnTYZaUgtMIK7YWs0ZlJ/PCG81n33dn88IbzUOB7f97MtIdKWPjKR1Yy3IQM+ygThEpcbsYMTmRE2oBAh2K6KSk+hi9PH8Ft04azqeoIz67bzZ/f38Pz63czcVgK86cO57oLskiOjwl0qMa0yc4cgowV2gsvIsLk4YP42c0TWffdOfzgcwWcqm9k4SsfcdEPS/jnpe+zerunfpYxwcTOHILMG+XVTqE9e1Z0uElJiOGOS/K4fUYuH+w5yksbqyjatI//3bSPrJR4bpyczRemDGNURmKgQzXGkkOwKXa5yUyK44LslECHYnqJiDApZyCTcgby/esKWFlWzUsbq3jsrR08+uYOLhw+kM9fOIxrz8+yx8KagLHkEETqGhp5q7yG6ydlW6G9CBEfE8V1F2Rx3QVZVB87w182ee5y+v5fNvODoi1cmp/O9ROHctWEIXa3k+lT9tcWRP7uFNq7ysYbIlJmcjwLZo7instGUrb/OEUf7OOvH+zj/mUfEBf9EVeMy+T6iUP5zLhMK6liep0lhyBS4hTau3hUWqBDMQEkIhQMTaZgaDLfmTuW93bX8tcP9vN/H+7ntc0HSIyL5qqCwXx2YhaXjE4nLtoShfE/Sw5BornQ3uVjrNCe+YSIMGVEKlNGpPL968aztvIwf/1gH69t3s8r7+8lMS6az4zL5JrzhnD5mAwG2KUn4yc+/SWJyFzgESAKeEJVf9xieRzwNDAFOATcoqq7nGULgbuARuCbqrrcxz5/BdypqhFx68ZHez2F9uaMt0tKpm3RUf24ND+dS/PT+Z8bzuPvOw6yfMsBVmxx89cP9hEX3Y+ZYzKYO2EIc8YPJqW/fYfCdF+nyUFEooDFwJXAHmCDiBSpqsur2V1AraqOFpH5wCLgFhEpAOYDE4ChQImIjHHWabdPESkEBvllD0NEsctNVD+xQnvGJ7HR/c6V7PjhDcqGXYd5ffMBlm85QLHLTXQ/4eJRaVw1YQizx2UydGBCoEM2IcaXM4epQIWqVgKIyFJgHuCdHOYBP3CmXwJ+I55qY/OApapaB+wUkQqnP9rr00lGPwW+BNzYg30LKcUuN4UjBjHIbl00XRTVT5g+Mo3pI9P4r88V8OGeo7y2+QCvb97Pf/xlM/8BjM9KZva4TK4Yn8nEYQOJsrvhTCd8SQ7ZQJXX6z3AtPbaqGqDiBwF0pz5a1usm+1Mt9fnfUCRqu7vqJqliCwAFgAMHz7ch90IXrsPnaLcbYX2TM+JCBNzBjIxZyDfmTuWHTUnWbXVTUlZNb99awe/eaOCtAGxzBqbyezxmVyWn06SlfAwbQiq0SsRGQrcDMzqrK2qPg48Dp6S3b0bWe8qLvMU2rvKvhVt/EhEGJ2ZyOjMRBbMHMWRU/W8ta2GVVurKSlz8/J7e4juJ0wbmcqsMZnMHJPBmMGJVmLcAL4lh71AjtfrYc68ttrsEZFoIAXPwHRH67Y1fzIwGqhw/kD7i0iFqo72aW9CVLHrAGMHJ9kziU2vGtg/lnmTspk3KZuGxibe232ElVvdrCqr5qFXy3jo1TIyk+K4LD+DmWPSuXR0OmmJcYEO2wSIL8lhA5AvInl43sDn4xkP8FYE3A6sAW4CVqmqikgR8JyI/ALPgHQ+sB6QtvpU1S3AuY/PInIi3BPDkVP1bNhVy9cvHxnoUEwEiY7qx9S8VKbmpbLwmvHsPXKad7bX8Pb2g+fOKgDOy072JIv8DKaMGGSPQY0gnSYHZwzhPmA5nttOl6jqFhF5EChV1SLgSeAZZ8D5MJ43e5x2y/AMXjcA96pqI0Bbffp/94Lfqq1WaM8EXvbABG65aDi3XDScxiblo71HWb2thtXbD/L7tyv57Zs76B8bxfSRaVw8Mo2LR6UxPivZBrbDmD0mNMC+8aeNbPy4lrULZ1s9JROUjp85y5odh1i9/SDvVBxk58GTgKfK7NS81HPJYuzgJPsbDjH2mNAgdeZsI29tq+GGyVZozwSvpPgYrpowhKsmeM5u9x89zdrKQ6zZcYg1lYcodp5cmDoglml5qVw8ynN2MTrTBrdDmSWHAFpTeYhT9Y32YB8TUrJSErhx8jBunDwMgD21p1iz4xBrKw+ztvIQr20+AEB6YhwX5Q6iMDeVi3IHUZCVTHSUjVmECksOAVTscjMgNooZVmjPhLBhg/pzc2F/bi7MQVWpOnyaNZUHWVt5mA27Dp9LFv1jo5g8fCCFI1K5KDeVycMHWi2oIGa/mQBpalJKXG5mjsmwqpombIgIw9P6MzzNM7gNnstQpbtqKd11mA27avnVqu2oer7ZXZCVTGHuIC7KTaVwxCAyk+MDvAemmSWHAPlw71Gqj9fZJSUT9rJSEvjcxAQ+N3EoAMfOnOX93UecZHGY59fv5g/v7gI8d01NyhnI5OGeJ+Wdl51iVYoDxJJDgBS7DlihPRORkuNjuHxMBpePyQCgvqGJLfuOsvHjWt6vOsKm3Uf420f7AYjuJ4zPSj73WNXJwweSlz7ABrr7gCWHAClxVXNR7iAG9rdCeyayxUb3Y/LwQUwe/kkh5urjZ9i0+wibqjw/r7y3h2fWfgx4bqGdmDOQyTkDmZiTwnnZKWQm2eUof7PkEADNhfb+47MFgQ7FmKCUmRT/qdtnG5uUiuoTbKqq5X0nafx61XaanK9pDU6O4/xsT6I43/mx8YueseQQACtcnrs3rrQH+xjjk6h+wtghSYwdknRuoPtEXQNb9h7lo71H2ez8u3JrNc3f681MapEwhqUw2BKGzyw5BECxy22F9ozpocS4aKaNTGPayE9uBT9R14Br37FPJYxV5Z8kjAwnYRRkJTM+K5nxWUmMSBtgZUDaYMmhj9WerGfDrsP846ywridoTEAkxkWfKyjY7GRdA2X7PQmjOWm8ta2GRueaVEJMFGOHJDE+K8lJGMmMG5IU8c+5sOTQx1ZtraZJsVtYjekjA+KiKcxNpTD3k4Rx5mwjFdUncO0/Rpnz8+pHB3h+/SfPIMtJTWD8kORzCaMgK5lhgxIiptSNJYc+VlLmPjd4ZowJjPiYKM5zxiOaqSr7j56hbP8xth44fi5xFJe5z12WGhAbxejBSYzJTGTM4CTyB3v+zUqJD7vbay059KHmQns3WqE9Y4KOiDB0YAJDByYw2+tmkdP1jZS7j1O2/xjlB46zzX2cN8preHHjnnNtkuKiGT04kbGDk8gfnMQYJ2lkJsWFbNKw5NCH1uzwFNqbY5eUjAkZCbFR576E5632ZD3b3MfZVn2C7e7jlB84zgqXm6UbPrk0lRwf7ZxheBLG6MxERmYkkpUcH/QfEC059KEVVmjPmLAxaEBsq7ulAA6eqGOb+zjb3SfO/fva5v08v/7suTYJMVGMzBjAyIxERmUMYFRGoud1eiIJscFRLsSSQx9palJKytxcPtYK7RkTztIT40hPjGPGqPRz81SVmhN1VNacZEfNCXZUn6TyoOdLff/34T68n7mWPTCBkU7CGJWZyKj0AYzKTOzzS1SWHPrIB3uOUGOF9oyJSCJCZlI8mUnxTG9xpnHmbCO7Dp1kR7UncVTWnGBHzUleLK3iZH3juXaJcdGMzBhAbtoA8tI/+Rk7JKlXihNacugjJWVuovoJnxlrhfaMMZ+Ij4li3JBkxg1J/tR8VcV9rO5TCWNHzQner6rlr15nGyu+PZMxg5P8HpdPyUFE5gKPAFHAE6r64xbL44CngSnAIeAWVd3lLFsI3AU0At9U1eUd9SkiTwKFgADbgDtU9UTPdjPwil1upuamWqE9Y4xPRIQhKfEMSYnnktHpn1p25mwjVYdPsfPgSUb0UqWFTp/ZJyJRwGLgGqAAuFVEWlaMuwuoVdXRwMPAImfdAmA+MAGYCzwqIlGd9PltVZ2oqhcAu4H7eriPAffxoZNsc5+wu5SMMX4RHxNF/uAkrpowpNfGMH15oOtUoEJVK1W1HlgKzGvRZh7wlDP9EjBbPCMn84ClqlqnqjuBCqe/dvtU1WMAzvoJgBLimh/AfpUlB2NMiPAlOWQDVV6v9zjz2myjqg3AUSCtg3U77FNE/gAcAMYBv24rKBFZICKlIlJaU1Pjw24EzgqXm3FDkshJtUJ7xpjQ4Ety6HOq+lVgKFAG3NJOm8dVtVBVCzMyMvo0vq6oPVlP6a7DdpeSMSak+JIc9gI5Xq+HOfPabCMi0UAKnoHp9tbttE9VbcRzuekLPsQYtKzQnjEmFPmSHDYA+SKSJyKxeAaYi1q0KQJud6ZvAlapqjrz54tInIjkAfnA+vb6FI/RcG7M4Xpga892MbCKXZ5Ce+cNtUJ7xpjQ0emtrKraICL3Acvx3Ha6RFW3iMiDQKmqFgFPAs+ISAVwGM+bPU67ZYALaADudc4IaKfPfsBTIpKM51bWD4Bv+HeX+86Zs428vd0K7RljQo9P33NQ1VeBV1vM+0+v6TPAze2s+xDwkI99NgGX+BJTKPj7joOcqm+0S0rGmJATlAPS4aLY5SYxLpqLrdCeMSbEWHLoJZ5Ce9VcPsYK7RljQo8lh15ihfaMMaHMkkMvKXZ5Cu3NGhu838Ewxpj2WHLoJVZozxgTyiw59IJdB0+yvfqEXVIyxoQsSw69oLnQniUHY0yosuTQC4rLrNCeMSa0WXLws8NOoT0rz22MCWWWHPysudCePdjHGBPKLDn4WbHrAEOS4zk/2wrtGWNClyUHPzpztpG3tx1kTkEmnqKyxhgTmiw5+NHfdxzk9NlGriwYEuhQjDGmRyw5+FFzob3pI1MDHYoxxvSIJQc/sUJ7xphwYsnBTzZZoT1jTBix5OAnzYX2PjM2M9ChGGNMj1ly8JNil5tpeamk9I8JdCjGGNNjlhz8YOfBk1RYoT1jTBjxKTmIyFwRKReRChF5oI3lcSLygrN8nYjkei1b6MwvF5GrO+tTRJ515m8WkSUiEvQfxUucQntzxltyMMaEh06Tg4hEAYuBa4AC4FYRKWjR7C6gVlVHAw8Di5x1C4D5wARgLvCoiER10uezwDjgfCABuLtHe9gHil1WaM8YE158OXOYClSoaqWq1gNLgXkt2swDnnKmXwJmi+crwvOApapap6o7gQqnv3b7VNVX1QGsB4b1bGsDbgoAAA0nSURBVBd71+GT9ZR+bIX2jDHhxZfkkA1Ueb3e48xrs42qNgBHgbQO1u20T+dy0leA19sKSkQWiEipiJTW1NT4sBu9Y2WZmybFvhVtjAkrwTwg/Sjwtqqubmuhqj6uqoWqWpiREbjnNBe73GSlxHNednLAYjDGGH/zJTnsBXK8Xg9z5rXZRkSigRTgUAfrdtiniPwXkAHc78tOBMqZs42s3n6QOeMHW6E9Y0xY8SU5bADyRSRPRGLxDDAXtWhTBNzuTN8ErHLGDIqA+c7dTHlAPp5xhHb7FJG7gauBW1W1qWe717verWgutGfjDcaY8BLdWQNVbRCR+4DlQBSwRFW3iMiDQKmqFgFPAs+ISAVwGM+bPU67ZYALaADuVdVGgLb6dDb5GPAxsMb5NP6Kqj7otz32o+ZCe9Os0J4xJsyI5wN+aCssLNTS0tI+3WZTkzL1/61k2shUFn/pwj7dtjHG+IOIbFTVwraWBfOAdFB7v+oIB0/U2S2sxpiwZMmhm0rK3ET3E2ZZoT1jTBiy5NBNxS4300amkpIQ9NU9jDGmyyw5dMO5QntWS8kYE6YsOXRDsesAAHNsvMEYE6YsOXRDscvN+Kxkhg2yQnvGmPBkyaGLDp2oY+PHtfbFN2NMWLPk0EWrtlbTpNgtrMaYsGbJoYuaC+1NGGqF9owx4cuSQxdYoT1jTKSw5NAF72y3QnvGmMhgyaELil1ukuKimT4yLdChGGNMr7Lk4KPGJmXlVjeXj80gNtoOmzEmvNm7nI82VR3h4Il6u6RkjIkIlhx8VOyyQnvGmMhhycFHxa4DVmjPGBMxLDn4oLLmBDtqTlqhPWNMxLDk4INilxuwQnvGmMhhycEHJWVuCqzQnjEmglhy6IQV2jPGRCKfkoOIzBWRchGpEJEH2lgeJyIvOMvXiUiu17KFzvxyEbm6sz5F5D5nnopIes92r+dWOoX2LDkYYyJJp8lBRKKAxcA1QAFwq4gUtGh2F1CrqqOBh4FFzroFwHxgAjAXeFREojrp811gDvBxD/fNL4pdboZaoT1jTITx5cxhKlChqpWqWg8sBea1aDMPeMqZfgmYLZ7KdPOApapap6o7gQqnv3b7VNX3VXVXD/fLL07XN7J6ew1zCqzQnjEmsviSHLKBKq/Xe5x5bbZR1QbgKJDWwbq+9NkhEVkgIqUiUlpTU9OVVX32TsVBzpxtsktKxpiIE7ID0qr6uKoWqmphRkZGr2yjxCm0Ny3PCu0ZYyKLL8lhL5Dj9XqYM6/NNiISDaQAhzpY15c+A6q50N6scZlWaM8YE3F8edfbAOSLSJ6IxOIZYC5q0aYIuN2ZvglYparqzJ/v3M2UB+QD633sM6A2VdVy8EQ9c8ZbLSVjTOTpNDk4Ywj3AcuBMmCZqm4RkQdF5Hqn2ZNAmohUAPcDDzjrbgGWAS7gdeBeVW1sr08AEfmmiOzBczbxoYg84b/d9d0KK7RnjIlg4vmAH9oKCwu1tLTUr31e8fM3GZqSwJ/unubXfo0xJliIyEZVLWxrmV1Mb8OOmhNU1py0u5SMMRHLkkMbSqzQnjEmwllyaEOxy82EoclkD0wIdCjGGBMQlhxaOHiijo27a5ljz24wxkQwSw4trCqrRq3QnjEmwllyaGGFy032wAQrtGeMiWiWHLycrm/knYoa5ozPtEJ7xpiIZsnByyeF9oYEOhRjjAkoSw5eil0HSIqPZtrI1ECHYowxAWXJwdHYpKwsq2bW2ExiouywGGMim70LOt7fXcuhk/V2l5IxxmDJ4Zxil5uYKGHW2N55NoQxxoQSSw6O4jI300emkRwfE+hQjDEm4Cw5YIX2jDGmJUsOeC4pAcy2khnGGANYcgCs0J4xxrQU8cmh5ngd7+2utUtKxhjjJeKTw6qtbiu0Z4wxLUR8cih2VZM9MIGCLCu0Z4wxzXxKDiIyV0TKRaRCRB5oY3mciLzgLF8nIrleyxY688tF5OrO+hSRPKePCqfP2J7tYvuaC+1dWTDYCu0ZY4yXTpODiEQBi4FrgALgVhEpaNHsLqBWVUcDDwOLnHULgPnABGAu8KiIRHXS5yLgYaevWqfvXrF6ew1nzjbZg32MMaYFX84cpgIVqlqpqvXAUmBeizbzgKec6ZeA2eL5KD4PWKqqdaq6E6hw+muzT2edK5w+cPq8ofu717Fil9sK7RljTBt8SQ7ZQJXX6z3OvDbbqGoDcBRI62Dd9uanAUecPtrbFgAiskBESkWktKamxofdaC0vYwC3TRthhfaMMaaF6EAH0F2q+jjwOEBhYaF2p49/nDXarzEZY0y48OUj814gx+v1MGdem21EJBpIAQ51sG578w8BA50+2tuWMcaYXuZLctgA5Dt3EcXiGWAuatGmCLjdmb4JWKWq6syf79zNlAfkA+vb69NZ5w2nD5w+/7f7u2eMMaY7Or2spKoNInIfsByIApao6hYReRAoVdUi4EngGRGpAA7jebPHabcMcAENwL2q2gjQVp/OJr8DLBWRHwLvO30bY4zpQ+L5sB7aCgsLtbS0NNBhGGNMSBGRjapa2NYyu03HGGNMK5YcjDHGtGLJwRhjTCuWHIwxxrQSFgPSIlIDfNzN1dOBg34Mx18srq6xuLrG4uqacI1rhKpmtLUgLJJDT4hIaXuj9YFkcXWNxdU1FlfXRGJcdlnJGGNMK5YcjDHGtGLJwSneF4Qsrq6xuLrG4uqaiIsr4sccjDHGtGZnDsYYY1qx5GCMMaaViE4OIjJXRMpFpEJEHujlbeWIyBsi4hKRLSLyz878H4jIXhHZ5Pxc67XOQie2chG5urfiFpFdIvKRs/1SZ16qiBSLyHbn30HOfBGRXznb/lBELvTq53an/XYRub297fkY01ivY7JJRI6JyLcCdbxEZImIVIvIZq95fjtGIjLF+R1UOOtKD+L6qYhsdbb9ZxEZ6MzPFZHTXsfusc62394+djMuv/3uxFPuf50z/wXxlP7vblwveMW0S0Q29eXxkvbfGwL796WqEfmDp1T4DmAkEAt8ABT04vaygAud6SRgG1AA/AD41zbaFzgxxQF5TqxRvRE3sAtIbzHvJ8ADzvQDwCJn+lrgNUCA6cA6Z34qUOn8O8iZHuTH39UBYESgjhcwE7gQ2NwbxwjPc06mO+u8BlzTg7iuAqKd6UVeceV6t2vRT5vbb28fuxmX3353wDJgvjP9GPCN7sbVYvnPgf/sy+NF++8NAf37iuQzh6lAhapWqmo9sBSY11sbU9X9qvqeM30cKKOd52M75gFLVbVOVXcCFU7MfRX3POApZ/op4Aav+U+rx1o8T+7LAq4GilX1sKrWAsXAXD/FMhvYoaodfQu+V4+Xqr6N51klLbfZ42PkLEtW1bXq+Z/8tFdfXY5LVVfoJ89hX4vniYrt6mT77e1jl+PqQJd+d86n3iuAl/wZl9PvF4HnO+rD38erg/eGgP59RXJyyAaqvF7voeM3a78RkVxgMrDOmXWfc3q4xOs0tL34eiNuBVaIyEYRWeDMG6yq+53pA8DgAMTVbD6f/g8b6OPVzF/HKNuZ7o0Y78TzSbFZnoi8LyJvichlXvG2t/329rG7/PG7SwOOeCVAfx2vywC3qm73mtenx6vFe0NA/74iOTkEhIgkAi8D31LVY8BvgVHAJGA/ntPavnapql4IXAPcKyIzvRc6nzYCcs+zcy35euBFZ1YwHK9WAnmM2iMi38PzBMZnnVn7geGqOhm4H3hORJJ97c8P+xiUvzsvt/LpDyF9erzaeG/odl/+EMnJYS+Q4/V6mDOv14hIDJ5f/rOq+gqAqrpVtVFVm4Df4zmV7ig+v8etqnudf6uBPzsxuJ3T0ebT6Oq+jstxDfCeqrqdGAN+vLz46xjt5dOXfnoco4jcAXwWuM15Y8G5bHPImd6I53r+mE62394+dpkff3eH8FxKiW4xv9ucvj4PvOAVb58dr7beGzroq2/+vjoblAjXHzzPz67EMwDWPNg1oRe3J3iu9f2yxfwsr+lv47n2CjCBTw/SVeIZoPNr3MAAIMlr+u94xgp+yqcHw37iTF/HpwfD1usng2E78QyEDXKmU/1w3JYCXw2G40WLAUp/HiNaDxhe24O45uJ5bntGi3YZQJQzPRLPG0SH229vH7sZl99+d3jOJL0HpP+xu3F5HbO3AnG8aP+9IaB/X73yRhgqP3hG/bfh+UTwvV7e1qV4Tgs/BDY5P9cCzwAfOfOLWvwH+p4TWzledxf4M27nj/4D52dLc394ruuuBLYDJV5/ZAIsdrb9EVDo1dedeAYTK/B6Q+9BbAPwfEpM8ZoXkOOF53LDfuAsnmu2d/nzGAGFwGZnnd/gVC/oZlwVeK49N/+dPea0/YLzO94EvAd8rrPtt7eP3YzLb7875+92vbOvLwJx3Y3Lmf9H4Ost2vbJ8aL994aA/n1Z+QxjjDGtRPKYgzHGmHZYcjDGGNOKJQdjjDGtWHIwxhjTiiUHY4wxrVhyMMYY04olB2OMMa38fxchyuDs72BTAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"-0r09aMPX6Hw"},"source":["## Use harvard model to find problem"]},{"cell_type":"code","metadata":{"id":"iro_i22eX5Lw","executionInfo":{"status":"ok","timestamp":1617162762248,"user_tz":-540,"elapsed":490,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["# class EncoderDecoder(nn.Module):\n","#     \"\"\"\n","#     A standard Encoder-Decoder architecture. Base for this and many \n","#     other models.\n","#     \"\"\"\n","#     def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","#         super(EncoderDecoder, self).__init__()\n","#         self.encoder = encoder\n","#         self.decoder = decoder\n","#         self.src_embed = src_embed\n","#         self.tgt_embed = tgt_embed\n","#         self.generator = generator\n","        \n","#     def forward(self, src, tgt, src_mask, tgt_mask):\n","#         \"Take in and process masked src and target sequences.\"\n","#         return self.decode(self.encode(src, src_mask), src_mask,\n","#                             tgt, tgt_mask)\n","    \n","#     def encode(self, src, src_mask):\n","#         return self.encoder(self.src_embed(src), src_mask)\n","    \n","#     def decode(self, memory, src_mask, tgt, tgt_mask):\n","#         return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n","\n","# class Generator(nn.Module):\n","#     \"Define standard linear + softmax generation step.\"\n","#     def __init__(self, d_model, vocab):\n","#         super(Generator, self).__init__()\n","#         self.proj = nn.Linear(d_model, vocab)\n","\n","#     def forward(self, x):\n","#         return F.log_softmax(self.proj(x), dim=-1)\n","\n","# def clones(module, N):\n","#     \"Produce N identical layers.\"\n","#     return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n","\n","# class Encoder(nn.Module):\n","#     \"Core encoder is a stack of N layers\"\n","#     def __init__(self, layer, N):\n","#         super(Encoder, self).__init__()\n","#         self.layers = clones(layer, N)\n","#         self.norm = LayerNorm(layer.size)\n","        \n","#     def forward(self, x, mask):\n","#         \"Pass the input (and mask) through each layer in turn.\"\n","#         for layer in self.layers:\n","#             x = layer(x, mask)\n","#         return self.norm(x)\n","\n","# class LayerNorm(nn.Module):\n","#     \"Construct a layernorm module (See citation for details).\"\n","#     def __init__(self, features, eps=1e-6):\n","#         super(LayerNorm, self).__init__()\n","#         self.a_2 = nn.Parameter(torch.ones(features))\n","#         self.b_2 = nn.Parameter(torch.zeros(features))\n","#         self.eps = eps\n","\n","#     def forward(self, x):\n","#         mean = x.mean(-1, keepdim=True)\n","#         std = x.std(-1, keepdim=True)\n","#         return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","# class SublayerConnection(nn.Module):\n","#     \"\"\"\n","#     A residual connection followed by a layer norm.\n","#     Note for code simplicity the norm is first as opposed to last.\n","#     \"\"\"\n","#     def __init__(self, size, dropout):\n","#         super(SublayerConnection, self).__init__()\n","#         self.norm = LayerNorm(size)\n","#         self.dropout = nn.Dropout(dropout)\n","\n","#     def forward(self, x, sublayer):\n","#         \"Apply residual connection to any sublayer with the same size.\"\n","#         return x + self.dropout(sublayer(self.norm(x)))\n","\n","# class EncoderLayer(nn.Module):\n","#     \"Encoder is made up of self-attn and feed forward (defined below)\"\n","#     def __init__(self, size, self_attn, feed_forward, dropout):\n","#         super(EncoderLayer, self).__init__()\n","#         self.self_attn = self_attn\n","#         self.feed_forward = feed_forward\n","#         self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","#         self.size = size\n","\n","#     def forward(self, x, mask):\n","#         \"Follow Figure 1 (left) for connections.\"\n","#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","#         return self.sublayer[1](x, self.feed_forward)\n","\n","# class Decoder(nn.Module):\n","#     \"Generic N layer decoder with masking.\"\n","#     def __init__(self, layer, N):\n","#         super(Decoder, self).__init__()\n","#         self.layers = clones(layer, N)\n","#         self.norm = LayerNorm(layer.size)\n","        \n","#     def forward(self, x, memory, src_mask, tgt_mask):\n","#         for layer in self.layers:\n","#             x = layer(x, memory, src_mask, tgt_mask)\n","#         return self.norm(x)\n","\n","# class DecoderLayer(nn.Module):\n","#     \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","#     def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","#         super(DecoderLayer, self).__init__()\n","#         self.size = size\n","#         self.self_attn = self_attn\n","#         self.src_attn = src_attn\n","#         self.feed_forward = feed_forward\n","#         self.sublayer = clones(SublayerConnection(size, dropout), 3)\n"," \n","#     def forward(self, x, memory, src_mask, tgt_mask):\n","#         \"Follow Figure 1 (right) for connections.\"\n","#         m = memory\n","#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","#         x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","#         return self.sublayer[2](x, self.feed_forward)\n","\n","# def subsequent_mask(size):\n","#     \"Mask out subsequent positions.\"\n","#     attn_shape = (1, size, size)\n","#     subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","#     return torch.from_numpy(subsequent_mask) == 0\n","\n","# def attention(query, key, value, mask=None, dropout=None):\n","#     \"Compute 'Scaled Dot Product Attention'\"\n","#     d_k = query.size(-1)\n","#     scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","#              / math.sqrt(d_k)\n","#     if mask is not None:\n","#         scores = scores.masked_fill(mask == 0, -1e9)\n","#     p_attn = F.softmax(scores, dim = -1)\n","#     if dropout is not None:\n","#         p_attn = dropout(p_attn)\n","#     return torch.matmul(p_attn, value), p_attn\n","\n","# class MultiHeadedAttention(nn.Module):\n","#     def __init__(self, h, d_model, dropout=0.1):\n","#         \"Take in model size and number of heads.\"\n","#         super(MultiHeadedAttention, self).__init__()\n","#         assert d_model % h == 0\n","#         # We assume d_v always equals d_k\n","#         self.d_k = d_model // h\n","#         self.h = h\n","#         self.linears = clones(nn.Linear(d_model, d_model), 4)\n","#         self.attn = None\n","#         self.dropout = nn.Dropout(p=dropout)\n","        \n","#     def forward(self, query, key, value, mask=None):\n","#         \"Implements Figure 2\"\n","#         if mask is not None:\n","#             # Same mask applied to all h heads.\n","#             mask = mask.unsqueeze(1)\n","#         nbatches = query.size(0)\n","        \n","#         # 1) Do all the linear projections in batch from d_model => h x d_k \n","#         query, key, value = \\\n","#             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","#              for l, x in zip(self.linears, (query, key, value))]\n","        \n","#         # 2) Apply attention on all the projected vectors in batch. \n","#         x, self.attn = attention(query, key, value, mask=mask, \n","#                                  dropout=self.dropout)\n","        \n","#         # 3) \"Concat\" using a view and apply a final linear. \n","#         x = x.transpose(1, 2).contiguous() \\\n","#              .view(nbatches, -1, self.h * self.d_k)\n","#         return self.linears[-1](x)\n","\n","# class PositionwiseFeedForward(nn.Module):\n","#     \"Implements FFN equation.\"\n","#     def __init__(self, d_model, d_ff, dropout=0.1):\n","#         super(PositionwiseFeedForward, self).__init__()\n","#         self.w_1 = nn.Linear(d_model, d_ff)\n","#         self.w_2 = nn.Linear(d_ff, d_model)\n","#         self.dropout = nn.Dropout(dropout)\n","\n","#     def forward(self, x):\n","#         return self.w_2(self.dropout(F.relu(self.w_1(x))))\n","\n","# class Embeddings(nn.Module):\n","#     def __init__(self, d_model, vocab):\n","#         super(Embeddings, self).__init__()\n","#         self.lut = nn.Embedding(vocab, d_model)\n","#         self.d_model = d_model\n","\n","#     def forward(self, x):\n","#         return self.lut(x) * math.sqrt(self.d_model)\n","\n","# class PositionalEncoding(nn.Module):\n","#     \"Implement the PE function.\"\n","#     def __init__(self, d_model, dropout, max_len=5000):\n","#         super(PositionalEncoding, self).__init__()\n","#         self.dropout = nn.Dropout(p=dropout)\n","        \n","#         # Compute the positional encodings once in log space.\n","#         pe = torch.zeros(max_len, d_model)\n","#         position = torch.arange(0, max_len).unsqueeze(1)\n","#         div_term = torch.exp(torch.arange(0, d_model, 2) *\n","#                              -(math.log(10000.0) / d_model))\n","#         pe[:, 0::2] = torch.sin(position * div_term)\n","#         pe[:, 1::2] = torch.cos(position * div_term)\n","#         pe = pe.unsqueeze(0)\n","#         self.register_buffer('pe', pe)\n","        \n","#     def forward(self, x):\n","#         x = x + Variable(self.pe[:, :x.size(1)], \n","#                          requires_grad=False)\n","#         return self.dropout(x)\n","\n","# def make_model(src_vocab, tgt_vocab, N=6, \n","#                d_model=512, d_ff=2048, h=8, dropout=0.1):\n","#     \"Helper: Construct a model from hyperparameters.\"\n","#     c = copy.deepcopy\n","#     attn = MultiHeadedAttention(h, d_model)\n","#     ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","#     position = PositionalEncoding(d_model, dropout)\n","#     model = EncoderDecoder(\n","#         Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","#         Decoder(DecoderLayer(d_model, c(attn), c(attn), \n","#                              c(ff), dropout), N),\n","#         nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","#         nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","#         Generator(d_model, tgt_vocab))\n","    \n","#     print(model)\n","\n","#     # This was important from their code. \n","#     # Initialize parameters with Glorot / fan_avg.\n","#     for p in model.parameters():\n","#         if p.dim() > 1:\n","#             nn.init.xavier_uniform_(p)\n","#     return model"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czg4Glm3jfev"},"source":["## Define main function\n","\n"]},{"cell_type":"code","metadata":{"id":"ZZBkM66Lc7FG","executionInfo":{"status":"ok","timestamp":1617162763446,"user_tz":-540,"elapsed":867,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["def main(args):\n","    src, tgt = load_data(args.path)\n","\n","    src_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n","    src_vocab.load(os.path.join(args.path, 'vocab.en'))\n","    tgt_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n","    tgt_vocab.load(os.path.join(args.path, 'vocab.de'))\n","\n","    # TODO(completed): use these information.\n","    sos_idx = 0\n","    eos_idx = 1\n","    pad_idx = 2\n","    max_length = 50\n","\n","    # TODO: use these values to construct embedding layers\n","    src_vocab_size = len(src_vocab)\n","    tgt_vocab_size = len(tgt_vocab)\n","\n","    # Define training parameters\n","    model_dim = 512\n","    warmup_steps = 4000\n","\n","    # Define model\n","    transformer = Transformer(model_dim, src_vocab_size, tgt_vocab_size, max_length)\n","    for param in transformer.parameters():\n","        if param.dim() > 1:\n","            nn.init.xavier_uniform_(param)\n","    if args.test and args.model_path: # test\n","        transformer.load_state_dict(torch.load(args.model_path))\n","        transformer.eval()\n","\n","    # print(transformer.parameters())\n","    # for name, param in transformer.named_parameters():\n","    #     if param.requires_grad:\n","    #         print(name, param.data)\n","\n","    # Define optimizer\n","    step_num = 1\n","    learning_rate = get_learning_rate(model_dim, step_num, warmup_steps)\n","    optimizer = optim.Adam(transformer.parameters(), lr = learning_rate, betas = (0.9, 0.98), eps = 1e-8)\n","\n","    # Define loss function\n","    ## smoothing = 0.1\n","    ## train_loss_function = LabelSmoothingsLoss(tgt_vocab_size, smoothing, dim = -1, is_train = True)\n","    ## validation_loss_function = LabelSmoothingsLoss(tgt_vocab_size, smoothing, dim = -1, is_train = False)\n","    train_loss_function = nn.CrossEntropyLoss(ignore_index = pad_idx).to(device)\n","    validation_loss_function = nn.CrossEntropyLoss(ignore_index = pad_idx).to(device)\n","\n","    if not args.test:\n","        train_loader = get_loader(src['train'], tgt['train'], src_vocab, tgt_vocab, batch_size=args.batch_size, shuffle=True)\n","        valid_loader = get_loader(src['valid'], tgt['valid'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n","\n","        history = {\n","            \"loss\" : [],\n","            \"val_loss\" : [],\n","            \"accuracy\" : [],\n","            \"val_accuracy\" : []\n","        }\n","\n","        for epoch in range(1, args.epochs + 1):\n","            print(f\"Epoch {epoch}/{args.epochs}\")\n","            total_train_size, total_validation_size = 0, 0\n","            epoch_train_loss, epoch_validation_loss = 0, 0\n","            epoch_train_correct, epoch_validation_correct = 0, 0\n","\n","            # TODO(completed): train\n","            # for src_batch, tgt_batch in tqdm(train_loader):\n","            for src_batch, tgt_batch in train_loader:\n","                for g in optimizer.param_groups: # update learning rate first\n","                    g['lr'] = get_learning_rate(model_dim, step_num, warmup_steps)\n","                optimizer.zero_grad()\n","\n","                src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n","                tgt_mask = get_mask(torch.tensor(tgt_batch)[:, :-1], pad_idx, is_target = True)\n","\n","                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch)[:, :-1].to(device), src_mask.to(device), tgt_mask.to(device))\n","                loss = train_loss_function( \\\n","                    torch.tensor(prd_batch.transpose(-1, -2), dtype = torch.float, requires_grad = True).clone().to(device), \\\n","                    torch.tensor(tgt_batch)[:, 1:].clone().to(device) \\\n","                )\n","\n","                loss.backward()\n","                optimizer.step()\n","                step_num += 1\n","\n","                total_train_size += len(src_batch)\n","                epoch_train_loss += loss.data\n","                epoch_train_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).data[:, 1:].to(device)))\n","\n","                # print(\"\\n\\n\\n\\n\\n\\n\")\n","                # print(total_train_size)\n","                # print(epoch_train_loss)\n","                # print(epoch_train_correct)\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","            epoch_train_loss /= total_train_size\n","            epoch_train_accuracy = epoch_train_correct / total_train_size\n","\n","            history[\"loss\"].append(epoch_train_loss)    \n","            history[\"accuracy\"].append(epoch_train_accuracy)\n","\n","            # TODO: validation\n","            # for src_batch, tgt_batch in tqdm(valid_loader):\n","            for src_batch, tgt_batch in valid_loader:\n","                src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n","                tgt_mask = get_mask(torch.tensor(tgt_batch)[:, :-1], pad_idx, is_target = True)\n","\n","                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch)[:, :-1].to(device), src_mask.to(device), tgt_mask.to(device))\n","                loss = validation_loss_function( \\\n","                    torch.tensor(prd_batch.transpose(-1, -2), dtype = torch.float, requires_grad = True).clone().to(device), \\\n","                    torch.tensor(tgt_batch)[:, 1:].clone().to(device), \\\n","                )\n","\n","                total_validation_size += len(src_batch)\n","                epoch_validation_loss += loss.data\n","                epoch_validation_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).data[:, 1:].to(device)))\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","            epoch_validation_loss /= total_validation_size\n","            epoch_validation_accuracy = epoch_validation_correct / total_validation_size\n","\n","            history[\"val_loss\"].append(epoch_validation_loss)\n","            history[\"val_accuracy\"].append(epoch_validation_accuracy)\n","\n","            print(f\"loss : {epoch_train_loss:.6f}, val_loss : {epoch_validation_loss:.6f}\") \n","            print(f\"accuracy : {epoch_train_accuracy:.6f}, val_accuracy : {epoch_validation_accuracy:.6f}\")\n","\n","            transformer.save_model(args.output_path, epoch, epoch_train_loss, epoch_validation_loss)\n","        transformer.plot(args.output_path, history)\n","\n","    else: # test\n","        test_loader = get_loader(src['test'], tgt['test'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n","\n","        pred = []\n","        # for src_batch, tgt_batch in tqdm(test_loader):\n","        for src_batch, tgt_batch in test_loader:\n","            # TODO: predict pred_batch from src_batch with your model.\n","            result_batch = torch.tensor([sos_idx for _ in range(len(src_batch))]).unsqueeze(-1)\n","            for idx in range(max_length):\n","                if idx == max_length - 1:\n","                    pred_batch = torch.full((len(src_batch), 1), eos_idx)\n","                else:\n","                    src_mask = get_mask(torch.tensor(src_batch), pad_idx, is_target = False)\n","                    result_mask = get_mask(torch.tensor(result_batch), pad_idx, is_target = True)\n","\n","                    pred_batch = transformer(torch.tensor(src_batch).to(device), result_batch.to(device), src_mask.to(device), result_mask.to(device))[:, -1, :] # consider only last result\n","                    pred_batch[:, sos_idx] += -1e9 # exclude <sos>\n","                    pred_batch[:, pad_idx] += -1e9 # exclude <pad>\n","                    pred_batch = torch.argmax(pred_batch, dim = -1).unsqueeze(-1)\n","                result_batch = torch.cat((result_batch.to(device), pred_batch.to(device)), dim = -1)\n","            result_batch = result_batch[1:, :] # remove <sos> from the result\n","\n","            max_length = 0\n","            for batch_idx in range(result_batch.shape[0]):\n","                max_length = max(max_length, list(result_batch[batch_idx]).index(eos_idx)) # find <eos>\n","            result_batch = result_batch[:max_length, :].tolist()\n","            pred += seq2sen(result_batch, tgt_vocab)\n","\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        with open('results/pred.txt', 'w') as f:\n","            for line in pred:\n","                f.write('{}\\n'.format(line))\n","            f.close()\n","\n","        os.system('bash scripts/bleu.sh results/pred.txt multi30k/test.de.atok')"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cbk70xZjZdQ"},"source":["## Call main function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgIAJkvTjhAB","executionInfo":{"status":"ok","timestamp":1617162822541,"user_tz":-540,"elapsed":58730,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"10f92ef7-0839-441a-c4d9-a90cb2bd8c56"},"source":["args = easydict.EasyDict({\n","    \"path\": \"multi30k\",\n","    \"epochs\": 100,\n","    \"batch_size\": 128,\n","    \"test\": True,\n","    \"output_path\": \"resources\",\n","    \"model_path\": \"resources/weights_010_0.0663_0.0668.pt\"\n","})\n","\n","main(args)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"n5tcDKfhHwMm"},"source":["## Use console"]},{"cell_type":"code","metadata":{"id":"C4Cy1zZiGoa-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617162845751,"user_tz":-540,"elapsed":5304,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"11cb3d5a-1eae-4cd9-e8d9-16423afe32f2"},"source":["from kora import console\n","console.start()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Console URL: https://teleconsole.com/s/2112f2cb151b6384601bec8d8782243776124afa\n"],"name":"stdout"}]}]}