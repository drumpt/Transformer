{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPOjFocY68zyu6gP0rNWA4d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7jvG1JwnVI4f"},"source":["## Mount google drive and change directory"]},{"cell_type":"code","metadata":{"id":"di0fxpN9uD1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616718620641,"user_tz":-540,"elapsed":2904,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"d6c636e2-e457-4743-f562-392e0976ba5a"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = False)\n","\n","!git clone https://github.com/drumpt/NMT_practice.git/ /content/drive/My\\ Drive/Colab\\ Notebooks/CS495\\ Individual\\ Study/1회차\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/CS495\\ Individual\\ Study/1회차/NMT_practice\n","!pip install -r requirements.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","fatal: destination path '/content/drive/My Drive/Colab Notebooks/CS495 Individual Study/1회차' already exists and is not an empty directory.\n","/content/drive/My Drive/Colab Notebooks/CS495 Individual Study/1회차/NMT_practice\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.0+cu101)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.10.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 2)) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o4LUnjN2gquD"},"source":["## Import libraries and choose device"]},{"cell_type":"code","metadata":{"id":"cbhFmz90dJkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616718621012,"user_tz":-540,"elapsed":3265,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"8f310e36-736f-446a-d66c-315fed4cd938"},"source":["import os\n","import math\n","# import argparse\n","import easydict\n","import gc\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","from dataset.dataloader import load_data, get_loader\n","from dataset.field import Vocab\n","from utils import seq2sen\n","\n","if torch.cuda.is_available():\n","    torch.cuda.set_device(0)\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","print(f\"Use {device} for torch\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Use cuda for torch\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"snOczaHygMn1"},"source":["## Define Encoder"]},{"cell_type":"code","metadata":{"id":"DShYHtg1gNEP","executionInfo":{"status":"ok","timestamp":1616718621013,"user_tz":-540,"elapsed":3257,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        # TODO(completed) : num_identical_layers를 이용할 수 있을까? clone을 이용하면 됨.\n","        self.identical_layer1 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer2 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer3 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer4 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer5 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer6 = EncoderIdenticalLayer(self.model_dim, self.h).to(device)\n","\n","    def forward(self, x):\n","        x = self.identical_layer1(x)\n","        x = self.identical_layer2(x)\n","        x = self.identical_layer3(x)\n","        x = self.identical_layer4(x)\n","        x = self.identical_layer5(x)\n","        x = self.identical_layer6(x)\n","        return x\n","\n","class EncoderIdenticalLayer(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.w_i_Q_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","\n","        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048, bias = True).to(device)\n","        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512, bias = True).to(device)\n","        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device) # TODO(completed): 맞나?\n","\n","        self.dropout = nn.Dropout(p = 0.1).to(device)\n","        self.softmax = nn.Softmax(dim = 2).to(device) # TODO(completed): 이게 맞나? 2인 것 같다.\n","        self.relu = nn.ReLU().to(device)\n","\n","    def forward(self, x):\n","        ### Sublayer 1\n","        # Multi-Head Attention\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list[i](x),\n","                self.w_i_K_list[i](x).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            out_softmax = self.w_i_V_list[i](x)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2) # TODO(completed): 맞나?\n","        multi_head_attention_output = self.dropout(self.w_O(concatenated_x))\n","\n","        # Add & Layer Normalization\n","        multi_head_attention_output += x\n","        multi_head_attention_output = self.layer_normalization1(multi_head_attention_output)\n","\n","        ### Sublayer 2\n","        # Feed-Forward Network\n","        positionwise_output_list = [] # (0, 2, 1)\n","        for position in range(multi_head_attention_output.shape[1]): # token length\n","            positionwise_input = multi_head_attention_output[:, position, :]\n","            positionwise_output = self.relu(self.feed_forward_network1(positionwise_input))\n","            positionwise_output = self.feed_forward_network2(positionwise_output)\n","            positionwise_output_list.append(positionwise_output)\n","        ffn_output = self.dropout(torch.stack(positionwise_output_list, dim = 1))\n","\n","        # Add & Layer Normalization\n","        ffn_output += multi_head_attention_output\n","        ffn_output = self.layer_normalization2(ffn_output)\n","        return ffn_output"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbqLLbJQgXU_"},"source":["## Define Decoder"]},{"cell_type":"code","metadata":{"id":"lPIVhaJ0gX78","executionInfo":{"status":"ok","timestamp":1616718621700,"user_tz":-540,"elapsed":3941,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.identical_layer1 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer2 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer3 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer4 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer5 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","        self.identical_layer6 = DecoderIdenticalLayer(self.model_dim, self.h).to(device)\n","\n","    def forward(self, x, y):\n","        x = self.identical_layer1(x, y)\n","        x = self.identical_layer2(x, y)\n","        x = self.identical_layer3(x, y)\n","        x = self.identical_layer4(x, y)\n","        x = self.identical_layer5(x, y)\n","        x = self.identical_layer6(x, y)\n","        return x\n","\n","class DecoderIdenticalLayer(nn.Module):\n","    def __init__(self, model_dim, h):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.h = h\n","\n","        self.w_i_Q_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list_first = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O_first = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization1 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","\n","        self.w_i_Q_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_K_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_i_V_list_second = [nn.Linear(in_features = self.model_dim, out_features = self.model_dim // self.h, bias = False).to(device) for _ in range(self.h)]\n","        self.w_O_second = nn.Linear(in_features = self.h * (self.model_dim // self.h), out_features = self.model_dim, bias = False).to(device)\n","        self.layer_normalization2 = nn.LayerNorm(self.model_dim).to(device) # exclude batch dimension\n","        \n","        self.feed_forward_network1 = nn.Linear(in_features = 512, out_features = 2048, bias = True).to(device)\n","        self.feed_forward_network2 = nn.Linear(in_features = 2048, out_features = 512, bias = True).to(device)\n","        self.layer_normalization3 = nn.LayerNorm(self.model_dim).to(device)\n","\n","        self.dropout = nn.Dropout(p = 0.1).to(device)\n","        self.softmax = nn.Softmax(dim = 2).to(device)\n","        self.relu = nn.ReLU().to(device)\n","\n","    def forward(self, x, y): # TODO(completed): masking 구현.\n","        ### Sublayer 1\n","        # Masked Multi-Head Attention\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list_first[i](x),\n","                self.w_i_K_list_first[i](x).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            in_softmax = self.masking(in_softmax)\n","            out_softmax = self.w_i_V_list_first[i](x)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n","        multi_head_attention_output_first = self.dropout(self.w_O_first(concatenated_x))\n","\n","        # Add & Layer Normalization\n","        multi_head_attention_output_first += x\n","        multi_head_attention_output_first = self.layer_normalization1(multi_head_attention_output_first)\n","\n","        ## Sublayer 2\n","        # Multi-Head Attention\n","        \"\"\"\n","        queries : come from previous decoder layer\n","        keys, values : come from the output of the encoder\n","        \"\"\"\n","        splitted_x_list = []\n","        for i in range(self.h):\n","            in_softmax = torch.matmul(\n","                self.w_i_Q_list_second[i](multi_head_attention_output_first),\n","                self.w_i_K_list_second[i](y).transpose(1, 2)\n","            ) / math.sqrt(self.model_dim // self.h)\n","            out_softmax = self.w_i_V_list_second[i](y)\n","            splitted_x = torch.matmul(self.softmax(in_softmax), out_softmax)\n","            splitted_x_list.append(splitted_x)\n","        concatenated_x = torch.cat(splitted_x_list, dim = 2)\n","        multi_head_attention_output_second = self.dropout(self.w_O_second(concatenated_x))\n","\n","        # Masked Multi-Head Attention\n","        multi_head_attention_output_second += multi_head_attention_output_first\n","        multi_head_attention_output_second = self.layer_normalization2(multi_head_attention_output_second)\n","\n","        ### Sublayer 3\n","        # Feed-Forward Network\n","        positionwise_output_list = [] # (0, 2, 1)\n","        for position in range(multi_head_attention_output_second.shape[1]): # token length\n","            positionwise_input = multi_head_attention_output_second[:, position, :]\n","            positionwise_output = self.relu(self.feed_forward_network1(positionwise_input))\n","            positionwise_output = self.feed_forward_network2(positionwise_output)\n","            positionwise_output_list.append(positionwise_output)\n","        ffn_output = self.dropout(torch.stack(positionwise_output_list, dim = 1))\n","\n","        # Add & Layer Normalization\n","        ffn_output += multi_head_attention_output_second\n","        ffn_output = self.layer_normalization3(ffn_output)\n","        return ffn_output\n","\n","    def masking(self, x):\n","        masking_tensor = torch.triu(torch.empty(x.shape[1], x.shape[2]).fill_(float(\"-inf\")), diagonal = 1).to(device)\n","        for idx in range(x.shape[0]):\n","            x[idx] += masking_tensor\n","        return x"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOinDT4ygmn7"},"source":["## Define Transformer"]},{"cell_type":"code","metadata":{"id":"Jaef9wnGf9Co","executionInfo":{"status":"ok","timestamp":1616718621700,"user_tz":-540,"elapsed":3938,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class Transformer(nn.Module):\n","    def __init__(self, model_dim, src_vocab_size, tgt_vocab_size, max_length):\n","        super().__init__()\n","        self.src_vocab_size = src_vocab_size\n","        self.tgt_vocab_size = tgt_vocab_size\n","        self.max_length = max_length\n","        self.model_dim = model_dim\n","        self.h = 8\n","        self.positional_encoding_constants = self.get_positional_encoding_constants()\n","\n","        self.input_embedding = nn.Embedding(self.src_vocab_size, self.model_dim).to(device)\n","        self.output_embedding = nn.Embedding(self.tgt_vocab_size, self.model_dim).to(device)\n","        self.embedding_dropout = nn.Dropout(p = 0.1).to(device)\n","        self.encoder = Encoder(self.model_dim, self.h).to(device)\n","        self.decoder = Decoder(self.model_dim, self.h).to(device)\n","        self.final_linear = nn.Linear(self.model_dim, self.tgt_vocab_size).to(device)\n","        self.softmax = nn.Softmax(dim = 2).to(device)\n","\n","    def forward(self, x, y): # x : encoder_input, y : decoder_input\n","        encoder_input = self.embedding_dropout(self.positional_encoding(self.input_embedding(x) * math.sqrt(self.model_dim)))\n","        encoder_output = self.encoder(encoder_input)\n","        decoder_output = self.decoder(self.embedding_dropout(self.positional_encoding(self.output_embedding(y) * math.sqrt(self.model_dim))), encoder_output)\n","        final_output = self.softmax(self.final_linear(decoder_output))\n","        final_output = self.final_linear(decoder_output)\n","        return final_output\n","\n","    def positional_encoding(self, embedded_sentence): # TODO(completed): batch 단위가 아니라 문장 단위로 전달되는지 확인. 안 됨.\n","        for batch_idx in range(embedded_sentence.shape[0]):\n","            embedded_sentence[batch_idx] += self.positional_encoding_constants[:embedded_sentence.shape[1], :]\n","        return embedded_sentence\n","\n","    def get_positional_encoding_constants(self): # avoid redundant calculations\n","        positional_encoding_constants = []\n","        for pos in range(self.max_length):\n","            pos_constants = []\n","            for idx in range(self.model_dim):\n","                if idx % 2 == 0: # idx = 2 * i -> 2 * i = idx\n","                    pos_constants.append(math.sin(pos / 10000 ** (idx / self.model_dim)))\n","                else: # idx = 2 * i + 1 -> 2 * i = idx - 1\n","                    pos_constants.append(math.cos(pos / 10000 ** ((idx  - 1) / self.model_dim)))\n","            positional_encoding_constants.append(pos_constants)\n","        return torch.tensor(positional_encoding_constants).to(device)\n","    \n","    def save_model(self, output_path, epoch, loss, val_loss):\n","        if not os.path.exists(output_path):\n","            os.makedirs(output_path)\n","\n","        output_filename = os.path.join(output_path, f\"weights_{epoch:03d}_{loss:.4f}_{val_loss:.4f}.pt\")\n","        torch.save(self.state_dict(), output_filename)\n","        return output_filename\n","\n","    def plot(self, output_path, history):\n","        plt.subplot(2, 1, 1)\n","        plt.title('Accuracy versus Epoch')\n","        plt.plot(history['accuracy'])\n","        plt.plot(history['val_accuracy'])\n","        plt.legend(['accuracy', 'val_accuracy'], loc = 'upper right')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","\n","        plt.subplot(2, 1, 2)\n","        plt.title('Loss versus Epoch')\n","        plt.plot(history['loss'])\n","        plt.plot(history['val_loss'])\n","        plt.legend(['loss', 'val_loss'], loc = 'upper right')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(output_path, \"training_result.png\"))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PN9SGQ7g4na"},"source":["## Define loss function and some functions"]},{"cell_type":"code","metadata":{"id":"oLL55nlkg4VV","executionInfo":{"status":"ok","timestamp":1616718621701,"user_tz":-540,"elapsed":3937,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["class LabelSmoothingsLoss(nn.Module):\n","    def __init__(self, num_classes, smoothing, dim, is_train):\n","        super().__init__()\n","        self.confidence = 1 - smoothing\n","        self.smoothing = smoothing\n","        self.num_classes = num_classes\n","        self.dim = dim\n","        self.is_train = is_train\n","\n","    def forward(self, pred, target, pad_start_idx_list = None):\n","        pred = torch.log(pred.clone().detach().requires_grad_(True))\n","        true_dist = torch.zeros_like(pred)\n","        true_dist.fill_(self.smoothing / (self.num_classes - 1))\n","        true_dist.scatter_(self.dim, target.data.unsqueeze(self.dim), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim = self.dim))\n","\n","def get_pad_start_idx_list(tgt_batch):\n","    pad_start_idx_list = []\n","    for batch_idx in range(tgt_batch.shape[0]):\n","        try:\n","            pad_start_idx = list(tgt_batch[batch_idx]).index(2) # <pad>\n","        except ValueError:\n","            pad_start_idx = tgt_batch[batch_idx].shape[0]\n","        pad_start_idx_list.append(pad_start_idx)\n","    return pad_start_idx_list\n","\n","def get_learning_rate(model_dim, step_num, warmup_steps):\n","    return (model_dim ** (-0.5)) * min(step_num ** (-0.5), step_num * (warmup_steps ** (-1.5)))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czg4Glm3jfev"},"source":["## Define main function\n","\n"]},{"cell_type":"code","metadata":{"id":"ZZBkM66Lc7FG","executionInfo":{"status":"ok","timestamp":1616718976145,"user_tz":-540,"elapsed":937,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}}},"source":["def main(args):\n","    src, tgt = load_data(args.path)\n","\n","    src_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n","    src_vocab.load(os.path.join(args.path, 'vocab.en'))\n","    tgt_vocab = Vocab(init_token='<sos>', eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n","    tgt_vocab.load(os.path.join(args.path, 'vocab.de'))\n","\n","    # TODO(completed): use these information.\n","    sos_idx = 0\n","    eos_idx = 1\n","    pad_idx = 2\n","    max_length = 50\n","\n","    # TODO: use these values to construct embedding layers\n","    src_vocab_size = len(src_vocab)\n","    tgt_vocab_size = len(tgt_vocab)\n","\n","    # Define training parameters\n","    model_dim = 512\n","    warmup_steps = 4000\n","\n","    # Define model\n","    transformer = Transformer(model_dim, src_vocab_size, tgt_vocab_size, max_length)\n","    if args.test and args.model_path:\n","        transformer.load_state_dict(torch.load(args.model_path))\n","        transformer.eval()\n","\n","    # Define optimizer\n","    step_num = 1\n","    learning_rate = get_learning_rate(model_dim, step_num, warmup_steps)\n","    optimizer = optim.Adam(transformer.parameters(), lr = learning_rate, betas = (0.9, 0.98), eps = 1e-8)\n","\n","    # Define loss function\n","    ## smoothing = 0.1\n","    ## train_loss_function = LabelSmoothingsLoss(tgt_vocab_size, smoothing, dim = -1, is_train = True)\n","    ## validation_loss_function = LabelSmoothingsLoss(tgt_vocab_size, smoothing, dim = -1, is_train = False)\n","    train_loss_function = nn.CrossEntropyLoss(ignore_index = pad_idx).to(device)\n","    # train_loss_function.requires_grad = True\n","    validation_loss_function = nn.CrossEntropyLoss().to(device)\n","\n","    if not args.test:\n","        train_loader = get_loader(src['train'], tgt['train'], src_vocab, tgt_vocab, batch_size=args.batch_size, shuffle=True)\n","        valid_loader = get_loader(src['valid'], tgt['valid'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n","\n","        history = {\n","            \"loss\" : [],\n","            \"val_loss\" : [],\n","            \"accuracy\" : [],\n","            \"val_accuracy\" : []\n","        }\n","\n","        for epoch in range(args.epochs):\n","            print(f\"Epoch {epoch + 1}/{args.epochs}\")\n","            total_train_size, total_validation_size = 0, 0\n","            epoch_train_loss, epoch_validation_loss = 0, 0\n","            epoch_train_correct, epoch_validation_correct = 0, 0\n","\n","            # TODO(completed): train\n","            for src_batch, tgt_batch in tqdm(train_loader):\n","                for g in optimizer.param_groups: # update learning rate first\n","                    g['lr'] = get_learning_rate(model_dim, step_num, warmup_steps)\n","                optimizer.zero_grad()\n","\n","                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch).to(device))\n","\n","                loss = train_loss_function( \\\n","                    torch.tensor(prd_batch.transpose(-1, -2), dtype = torch.float, requires_grad = True).clone().to(device), \\\n","                    torch.tensor(tgt_batch).clone().to(device) \\\n","                )\n","                loss.backward()\n","                optimizer.step()\n","                step_num += 1\n","\n","                total_train_size += len(src_batch)\n","                epoch_train_loss += loss\n","                epoch_train_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).to(device)))\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","            epoch_train_loss /= total_train_size\n","            epoch_train_accuracy = epoch_train_correct / total_train_size\n","\n","            history[\"loss\"].append(epoch_train_loss)    \n","            history[\"accuracy\"].append(epoch_train_accuracy)\n","\n","            # TODO: validation\n","            for src_batch, tgt_batch in tqdm(valid_loader):\n","                prd_batch = transformer(torch.tensor(src_batch).to(device), torch.tensor(tgt_batch).to(device))\n","                loss = validation_loss_function( \\\n","                    torch.tensor(prd_batch.transpose(-1, -2), dtype = torch.float, requires_grad = True).clone().to(device), \\\n","                    torch.tensor(tgt_batch).clone().to(device), \\\n","                )\n","\n","                total_validation_size += len(src_batch)\n","                epoch_validation_loss += loss\n","                epoch_validation_correct += int(torch.sum(torch.argmax(torch.tensor(prd_batch).data, -1) == torch.tensor(tgt_batch).to(device)))\n","\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","            epoch_validation_loss /= total_validation_size\n","            epoch_validation_accuracy = epoch_validation_correct / total_validation_size\n","\n","            history[\"val_loss\"].append(epoch_validation_loss)\n","            history[\"val_accuracy\"].append(epoch_validation_accuracy)\n","\n","            print(f\"loss : {epoch_train_loss:.6f}, val_loss : {epoch_validation_loss:.6f}\") \n","            print(f\"accuracy : {epoch_train_accuracy:.6f}, val_accuracy : {epoch_validation_accuracy:.6f}\")\n","\n","            transformer.save_model(args.output_path, epoch + 1, epoch_train_loss, epoch_validation_loss)\n","        transformer.plot(args.output_path, history)\n","\n","    else: # test\n","        test_loader = get_loader(src['test'], tgt['test'], src_vocab, tgt_vocab, batch_size=args.batch_size)\n","\n","        pred = []\n","        for src_batch, tgt_batch in tqdm(test_loader):\n","            # TODO: predict pred_batch from src_batch with your model.\n","            result_batch = torch.tensor([sos_index for _ in range(len(src_batch))]).unsqueeze(-1)\n","            for idx in range(max_length):\n","                if idx == max_length - 1:\n","                    pred_batch = torch.full((len(src_batch), 1), eos_idx)\n","                else:\n","                    pred_batch = transformer(torch.tensor(src_batch).to(device), result_batch.to(device))[:, -1, :] # consider only last result\n","                    pred_batch[:, sos_index] += float(\"-inf\") # exclude <sos>\n","                    pred_batch[:, pad_index] += float(\"-inf\") # exclude <pad>\n","                    pred_batch = torch.argmax(pred_batch, dim = -1).unsqueeze(-1)\n","                result_batch = torch.cat((result_batch.to(device), pred_batch.to(device)), dim = -1)\n","            result_batch = result_batch[1:, :] # remove <sos> from the result\n","\n","            max_length = 0\n","            for batch_idx in range(result_batch.shape[0]):\n","                max_length = max(max_length, list(result_batch[batch_idx]).find(eos_index)) # find <eos>\n","            result_batch = result_batch[:max_length, :].tolist()\n","            pred += seq2sen(result_batch, tgt_vocab)\n","\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        with open('results/pred.txt', 'w') as f:\n","            for line in pred:\n","                f.write('{}\\n'.format(line))\n","            f.close()\n","\n","        os.system('bash scripts/bleu.sh results/pred.txt multi30k/test.de.atok')"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cbk70xZjZdQ"},"source":["## Call main function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"DgIAJkvTjhAB","executionInfo":{"status":"error","timestamp":1616720897493,"user_tz":-540,"elapsed":1779173,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"df1276bf-d3d7-4bdb-a65f-be24c5a49c52"},"source":["args = easydict.EasyDict({\n","    \"path\": \"multi30k\",\n","    \"epochs\": 50,\n","    \"batch_size\": 1,\n","    \"test\": False,\n","    \"output_path\": \"resources\",\n","    \"model_path\": \"\"\n","})\n","\n","main(args)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","1it [00:00,  9.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n"],"name":"stdout"},{"output_type":"stream","text":["24219it [29:37, 14.28it/s]"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-79f8cd1caf21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m })\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-077905a278a5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mprd_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_function\u001b[0m\u001b[0;34m(\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.76 GiB total capacity; 12.99 GiB already allocated; 3.75 MiB free; 13.72 GiB reserved in total by PyTorch)"]}]},{"cell_type":"markdown","metadata":{"id":"n5tcDKfhHwMm"},"source":["## Use console"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4Cy1zZiGoa-","executionInfo":{"status":"ok","timestamp":1616721575359,"user_tz":-540,"elapsed":3432,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"8be8769b-f18b-4325-97f9-654e8b8a2f9d"},"source":["from kora import console\n","console.start()"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Console URL: https://teleconsole.com/s/fb5f10c3f764a6f4980f7e501189c471d0175932\n"],"name":"stdout"}]}]}